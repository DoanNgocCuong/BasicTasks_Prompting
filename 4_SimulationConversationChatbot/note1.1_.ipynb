{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D·ª±a tr√™n m√¥ t·∫£ c·ªßa b·∫°n, ch√∫ng ta c√≥ th·ªÉ thi·∫øt k·∫ø h·ªá th·ªëng n√†y v·ªõi ba th√†nh ph·∫ßn ch√≠nh:\n",
    "\n",
    "### **1. Ki·∫øn tr√∫c t·ªïng quan**\n",
    "- **AI Generator (AI 2)**:\n",
    "  - M√¥ ph·ªèng ho·∫∑c t·∫°o c√¢u n√≥i ƒë·∫ßu v√†o (user input) d·ª±a tr√™n m·ªôt s·ªë k·ªãch b·∫£n ho·∫∑c ng·ªØ c·∫£nh nh·∫•t ƒë·ªãnh.\n",
    "- **API Responder (AI 1)**:\n",
    "  - Nh·∫≠n input t·ª´ AI Generator, sau ƒë√≥ tr·∫£ v·ªÅ ph·∫£n h·ªìi ph√π h·ª£p (response).\n",
    "- **Evaluation Agent**:\n",
    "  - ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi c·ªßa API Responder d·ª±a tr√™n c√°c ti√™u ch√≠ nh∆∞:\n",
    "    - ƒê·ªô ch√≠nh x√°c (Accuracy)\n",
    "    - S·ª± ph√π h·ª£p ng·ªØ c·∫£nh (Relevance)\n",
    "    - T√≠nh t·ª± nhi√™n (Fluency)\n",
    "    - M·ª©c ƒë·ªô h·ªØu √≠ch (Usefulness)\n",
    "\n",
    "### **2. Lu·ªìng ho·∫°t ƒë·ªông**\n",
    "1. **AI Generator** t·∫°o c√¢u h·ªèi ho·∫∑c c√¢u n√≥i.\n",
    "2. **API Responder** x·ª≠ l√Ω input v√† tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi.\n",
    "3. **Evaluation Agent** nh·∫≠n input t·ª´ AI Generator v√† response t·ª´ API Responder ƒë·ªÉ ƒë√°nh gi√°.\n",
    "4. K·∫øt qu·∫£ ƒë√°nh gi√° ƒë∆∞·ª£c l∆∞u tr·ªØ ho·∫∑c hi·ªÉn th·ªã.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. C√°ch tri·ªÉn khai**\n",
    "D∆∞·ªõi ƒë√¢y l√† c√°ch vi·∫øt m√¥ ph·ªèng b·∫±ng Python ƒë·ªÉ th·ª±c hi·ªán lu·ªìng ho·∫°t ƒë·ªông n√†y.\n",
    "\n",
    "#### **Setup**\n",
    "- API Responder c√≥ endpoint `https://api.example.com/respond`.\n",
    "- AI Generator s·ª≠ d·ª•ng m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ nh∆∞ GPT-4 ƒë·ªÉ t·∫°o input.\n",
    "- Evaluation Agent s·∫Ω ƒë√°nh gi√° d·ª±a tr√™n c√°c ti√™u ch√≠.\n",
    "\n",
    "#### **Code Python**\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "# Configurations\n",
    "openai.api_key = \"your_openai_api_key\"  # Replace with your OpenAI API key\n",
    "api_responder_url = \"https://api.example.com/respond\"  # API endpoint c·ªßa AI 1\n",
    "\n",
    "# Step 1: AI Generator (AI 2)\n",
    "def generate_input(prompt=\"Generate a user question\"):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-4\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Step 2: API Responder (AI 1)\n",
    "def get_api_response(user_input):\n",
    "    response = requests.post(api_responder_url, json={\"user_input\": user_input})\n",
    "    return response.json().get(\"response\", \"No response\")\n",
    "\n",
    "# Step 3: Evaluation Agent\n",
    "def evaluate_response(user_input, api_response):\n",
    "    # Example criteria for evaluation\n",
    "    criteria = {\n",
    "        \"accuracy\": \"Is the response factually correct?\",\n",
    "        \"relevance\": \"Is the response relevant to the input?\",\n",
    "        \"fluency\": \"Is the response grammatically correct and natural?\",\n",
    "        \"usefulness\": \"Is the response useful to the user?\"\n",
    "    }\n",
    "    evaluation = {}\n",
    "    for key, question in criteria.items():\n",
    "        eval_prompt = f\"\"\"\n",
    "        User input: {user_input}\n",
    "        API response: {api_response}\n",
    "        Question: {question}\n",
    "        Rate on a scale of 1-10 and provide a short explanation.\n",
    "        \"\"\"\n",
    "        eval_response = openai.Completion.create(\n",
    "            engine=\"gpt-4\",\n",
    "            prompt=eval_prompt,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        evaluation[key] = eval_response.choices[0].text.strip()\n",
    "    return evaluation\n",
    "\n",
    "# Orchestrate the flow\n",
    "def main():\n",
    "    user_input = generate_input()  # AI 2 generates input\n",
    "    api_response = get_api_response(user_input)  # AI 1 responds\n",
    "    evaluation = evaluate_response(user_input, api_response)  # Agent evaluates\n",
    "    print(\"User Input:\", user_input)\n",
    "    print(\"API Response:\", api_response)\n",
    "    print(\"Evaluation:\", evaluation)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. ƒêi·ªÉm n·ªïi b·∫≠t**\n",
    "- **AI Generator**:\n",
    "  - S·ª≠ d·ª•ng OpenAI GPT ho·∫∑c m√¥ h√¨nh kh√°c ƒë·ªÉ t·∫°o c√¢u h·ªèi ng·∫´u nhi√™n ho·∫∑c c√≥ ƒëi·ªÅu ki·ªán.\n",
    "- **API Responder**:\n",
    "  - C√≥ th·ªÉ l√† API ƒë√£ t√≠ch h·ª£p ho·∫∑c do b·∫°n x√¢y d·ª±ng.\n",
    "- **Evaluation Agent**:\n",
    "  - ƒê√°nh gi√° t·ª± ƒë·ªông b·∫±ng c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh AI ƒë·ªÉ ph√¢n t√≠ch.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. M·ªü r·ªông**\n",
    "1. **L∆∞u k·∫øt qu·∫£**: \n",
    "   - L∆∞u input, response, v√† ƒë√°nh gi√° v√†o c∆° s·ªü d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch th√™m.\n",
    "2. **ƒê√†o t·∫°o l·∫°i API Responder**:\n",
    "   - D·ª±a tr√™n c√°c ƒë√°nh gi√° k√©m, b·∫°n c√≥ th·ªÉ c·∫£i thi·ªán ho·∫∑c tinh ch·ªânh m√¥ h√¨nh.\n",
    "3. **Th√™m ƒë√°nh gi√° con ng∆∞·ªùi**:\n",
    "   - So s√°nh k·∫øt qu·∫£ AI ƒë√°nh gi√° v·ªõi con ng∆∞·ªùi ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c.\n",
    "\n",
    "B·∫°n c√≥ mu·ªën tri·ªÉn khai c·ª• th·ªÉ m·ªôt ph·∫ßn n√†o trong h·ªá th·ªëng n√†y kh√¥ng? üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input ƒë·ªÅ b√†i v√† research \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ ki·ªÉm tra ch·∫•t l∆∞·ª£ng c·ªßa m·ªôt API ph·∫£n h·ªìi trong h·ªá th·ªëng h·ªôi tho·∫°i, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c framework v√† c√¥ng c·ª• sau ƒë·ªÉ m√¥ ph·ªèng ƒë·∫ßu v√†o t·ª´ ng∆∞·ªùi d√πng, nh·∫≠n ph·∫£n h·ªìi t·ª´ API, v√† ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi ƒë√≥:\n",
    "\n",
    ":::contextList\n",
    "\n",
    "**Agent Evaluation**  \n",
    "M·ªôt framework s·ª≠ d·ª•ng AI ƒë·ªÉ ki·ªÉm tra c√°c t√°c nh√¢n ·∫£o. N√≥ tri·ªÉn khai m·ªôt t√°c nh√¢n LLM (evaluator) ƒë·ªÉ ƒëi·ªÅu ph·ªëi c√°c cu·ªôc h·ªôi tho·∫°i v·ªõi t√°c nh√¢n c·ªßa b·∫°n (target) v√† ƒë√°nh gi√° c√°c ph·∫£n h·ªìi trong qu√° tr√¨nh h·ªôi tho·∫°i. \n",
    ":::\n",
    "\n",
    ":::contextList\n",
    "\n",
    "**LangGraph's Chat Bot Benchmarking using Simulation**  \n",
    "C√¥ng c·ª• n√†y cho ph√©p t·∫°o ra c√°c m√¥ ph·ªèng h·ªôi tho·∫°i gi·ªØa chatbot v√† ng∆∞·ªùi d√πng gi·∫£ l·∫≠p, gi√∫p ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa chatbot trong c√°c k·ªãch b·∫£n kh√°c nhau. \n",
    ":::\n",
    "\n",
    ":::contextList\n",
    "\n",
    "**Botium**  \n",
    "M·ªôt framework m√£ ngu·ªìn m·ªü h·ªó tr·ª£ ki·ªÉm th·ª≠ t·ª± ƒë·ªông cho c√°c n·ªÅn t·∫£ng chatbot kh√°c nhau. N√≥ cung c·∫•p c√°c c√¥ng c·ª• ƒë·ªÉ m√¥ ph·ªèng h·ªôi tho·∫°i v√† ƒë√°nh gi√° ph·∫£n h·ªìi c·ªßa chatbot. \n",
    ":::\n",
    "\n",
    ":::contextList\n",
    "\n",
    "**BotSIM**  \n",
    "M·ªôt framework m√¥ ph·ªèng end-to-end cho c√°c h·ªá th·ªëng h·ªôi tho·∫°i theo nhi·ªám v·ª•. BotSIM gi√∫p t·∫°o ra c√°c k·ªãch b·∫£n h·ªôi tho·∫°i ƒë·ªÉ ki·ªÉm th·ª≠ v√† ƒë√°nh gi√° chatbot. \n",
    ":::\n",
    "\n",
    ":::contextList\n",
    "\n",
    "**LangGraphJS's Agent Simulation Evaluation**  \n",
    "C√¥ng c·ª• n√†y cho ph√©p t·∫°o ra c√°c m√¥ ph·ªèng gi·ªØa c√°c t√°c nh√¢n AI, gi√∫p ƒë√°nh gi√° kh·∫£ nƒÉng t∆∞∆°ng t√°c v√† hi·ªáu su·∫•t c·ªßa ch√∫ng trong c√°c k·ªãch b·∫£n ph·ª©c t·∫°p. \n",
    ":::\n",
    "\n",
    "Vi·ªác s·ª≠ d·ª•ng c√°c framework n√†y s·∫Ω gi√∫p b·∫°n m√¥ ph·ªèng c√°c h·ªôi tho·∫°i gi·ªØa ng∆∞·ªùi d√πng v√† API, ƒë·ªìng th·ªùi ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi c·ªßa API m·ªôt c√°ch hi·ªáu qu·∫£. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/awslabs/agent-evaluation?utm_source=chatgpt.com\n",
    "- https://langchain-ai.github.io/langgraph/tutorials/chatbot-simulation-evaluation/langsmith-agent-simulation-evaluation/?utm_source=chatgpt.com\n",
    "- https://research.aimultiple.com/chatbot-testing-frameworks/?utm_source=chatgpt.com\n",
    "- https://arxiv.org/abs/2211.11982?utm_source=chatgpt.com\n",
    "- https://langchain-ai.github.io/langgraphjs/tutorials/chatbot-simulation-evaluation/agent-simulation-evaluation/?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend best framework, that simple framework, popular and high performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Response here. \n",
    "\n",
    "```\n",
    "curl --location 'http://103.253.20.13:9400/personalized-ai-coach/api/v1/bot/initConversation' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data '{\n",
    "    \"bot_id\": 3,\n",
    "    \"conversation_id\": \"123455677\",\n",
    "    \"input_slots\": {}\n",
    "}'\n",
    "```\n",
    "conversation_id we random for each conversation \n",
    "\n",
    "When start with \n",
    "```\n",
    "curl --location 'http://103.253.20.13:9400/personalized-ai-coach/api/v1/bot/webhook' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data '{\n",
    "    \"conversation_id\": \"123455677\",\n",
    "    \"message\": \"s·∫µn s√†ng\"\n",
    "}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the example code and your current implementation, I'll suggest some updates to improve your simulation conversation chatbot. Here are the key changes:\n",
    "\n",
    "1. First, let's update the evaluation logic in `agent_evaluation.py`:\n",
    "\n",
    "````python:4_SimulationConversationChatbot/agent_evaluation.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    score: int = Field(description=\"Score from 1-10\")\n",
    "    reasoning: str = Field(description=\"Detailed explanation of the evaluation\")\n",
    "\n",
    "def evaluate_response(api_key, user_input, api_response):\n",
    "    # Create evaluation prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are evaluating an AI assistant's response quality.\"),\n",
    "        MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "        (\"system\", \"Rate the response quality from 1-10 and explain why.\")\n",
    "    ])\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ChatOpenAI(api_key=api_key, model=\"gpt-4\").with_structured_output(EvaluationResult)\n",
    "    \n",
    "    # Format conversation for evaluation\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        {\"role\": \"assistant\", \"content\": api_response}\n",
    "    ]\n",
    "    \n",
    "    # Get evaluation\n",
    "    result = evaluator.invoke({\"conversation\": conversation})\n",
    "    return result\n",
    "````\n",
    "\n",
    "2. Update the AI generator to create more focused test cases:\n",
    "\n",
    "````python:4_SimulationConversationChatbot/ai_generator.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def generate_input(api_key, prompt=None):\n",
    "    if prompt is None:\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Generate a realistic user question to test an AI assistant.\n",
    "            Focus on common customer service scenarios.\n",
    "            The question should be clear and specific.\"\"\"),\n",
    "            (\"user\", \"Generate a test question\")\n",
    "        ])\n",
    "    \n",
    "    llm = ChatOpenAI(api_key=api_key, model=\"gpt-4\")\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.strip()\n",
    "````\n",
    "\n",
    "3. Update the main simulation flow:\n",
    "\n",
    "````python:4_SimulationConversationChatbot/main.py\n",
    "from api_responder import generate_conversation_id, init_conversation, send_message\n",
    "from ai_generator import generate_input\n",
    "from agent_evaluation import evaluate_response\n",
    "import json\n",
    "\n",
    "def simulate_conversation(openai_api_key, num_turns=3):\n",
    "    # Initialize conversation\n",
    "    conversation_id = generate_conversation_id()\n",
    "    init_response = init_conversation(BOT_ID, conversation_id, INIT_URL)\n",
    "    \n",
    "    results = []\n",
    "    for _ in range(num_turns):\n",
    "        # Generate user input\n",
    "        user_input = generate_input(api_key=openai_api_key)\n",
    "        print(f\"\\nUser: {user_input}\")\n",
    "        \n",
    "        # Get API response\n",
    "        api_response = send_message(conversation_id, user_input, WEBHOOK_URL)\n",
    "        print(f\"Assistant: {api_response['message']}\")\n",
    "        \n",
    "        # Evaluate response\n",
    "        evaluation = evaluate_response(\n",
    "            api_key=openai_api_key,\n",
    "            user_input=user_input,\n",
    "            api_response=api_response[\"message\"]\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"turn\": _ + 1,\n",
    "            \"user_input\": user_input,\n",
    "            \"api_response\": api_response[\"message\"],\n",
    "            \"evaluation_score\": evaluation.score,\n",
    "            \"evaluation_reasoning\": evaluation.reasoning\n",
    "        })\n",
    "        \n",
    "        print(f\"Evaluation Score: {evaluation.score}/10\")\n",
    "        print(f\"Reasoning: {evaluation.reasoning}\")\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"simulation_results_{conversation_id}.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    OPENAI_API_KEY = \"your_openai_api_key\"\n",
    "    results = simulate_conversation(OPENAI_API_KEY)\n",
    "````\n",
    "\n",
    "Key improvements made:\n",
    "\n",
    "1. **Structured Evaluation**: Using Pydantic models for better type safety and structured output\n",
    "2. **Better Prompting**: More focused prompts for generating test cases\n",
    "3. **Multi-turn Testing**: Support for multiple conversation turns\n",
    "4. **Result Storage**: Saving conversation results to JSON for later analysis\n",
    "5. **Improved Logging**: Better console output during simulation\n",
    "\n",
    "To use this updated version:\n",
    "\n",
    "1. Make sure you have the required dependencies:\n",
    "```bash\n",
    "pip install langchain-openai pydantic\n",
    "```\n",
    "\n",
    "2. Update your API keys and endpoints in the configuration\n",
    "\n",
    "3. Run the simulation:\n",
    "```python\n",
    "from main import simulate_conversation\n",
    "results = simulate_conversation(OPENAI_API_KEY)\n",
    "```\n",
    "\n",
    "This will generate test conversations, evaluate responses, and save the results for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
