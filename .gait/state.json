{
  "panelChats": [],
  "inlineChats": [
    {
      "inline_chat_id": "1ad63d8f-0493-4868-ac5a-b61ac6a2645e",
      "file_diff": [
        {
          "file_path": "5_TuningPrompting/HowRun.md",
          "diffs": [
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "python PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py"
            },
            {
              "count": 23,
              "added": true,
              "removed": false,
              "value": "# Basic run with default parameters\r\npython PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py\r\n\r\n# Run with custom input/output files\r\npython PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py --input-file custom_input.xlsx --output-file custom_output.xlsx\r\n\r\n# Run with specific number of rows\r\npython PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py --num-rows 100\r\n\r\n# Run with custom sheet name\r\npython PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py --sheet \"Sheet1\"\r\n\r\n# Run with custom batch size and workers\r\npython PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py --batch-size 8 --max-workers 6\r\n\r\n# Run with all custom parameters\r\npython PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py \\\r\n  --input-file data.xlsx \\\r\n  --output-file results.xlsx \\\r\n  --sheet \"MySheet\" \\\r\n  --num-rows 50 \\\r\n  --batch-size 10 \\\r\n  --max-workers 8"
            }
          ]
        }
      ],
      "selection": null,
      "timestamp": "2025-03-18T23:54:24.676Z",
      "prompt": "def parse_arguments():\n    parser = argparse.ArgumentParser(description='Process conversations with OpenAI API')\n    parser.add_argument('--num-rows', type=int, default=None,\n                      help='Number of rows to process (default: all rows)')\n    parser.add_argument('--input-file', type=str, default='input_data.xlsx',\n                      help='Input Excel file path (default: input_data.xlsx)')\n    parser.add_argument('--output-file', type=str, default='output_data_v2.xlsx',\n                      help='Output Excel file path (default: output_data_v2.xlsx)')\n    parser.add_argument('--sheet', type=str, default='Trang t√≠nh1',\n                      help='Excel sheet name to process (default: Trang t√≠nh1)')\n    parser.add_argument('--batch-size', type=int, default=4,\n                      help='Number of items to process in each batch (default: 4)')\n    parser.add_argument('--max-workers', type=int, default=4,\n                      help='Maximum number of worker threads (default: 4)')\n    return parser.parse_args()\n\n\nupdate c√°c ki·ªÉu run ",
      "parent_inline_chat_id": null
    },
    {
      "inline_chat_id": "78ad52cf-6415-4683-b2ac-9bbd02556b93",
      "file_diff": [
        {
          "file_path": "7_eval_compareFastResponse/processed.py",
          "diffs": [
            {
              "count": 36,
              "added": false,
              "removed": false,
              "value": "import json\r\nimport pandas as pd\r\nimport os\r\nfrom typing import List, Dict, Any\r\n\r\nclass ConversationProcessor:\r\n    def __init__(self):\r\n        pass\r\n    \r\n    def load_json_data(self, filepath: str) -> Dict[Any, Any]:\r\n        \"\"\"\r\n        Load d·ªØ li·ªáu t·ª´ file JSON\r\n        \"\"\"\r\n        try:\r\n            with open(filepath, 'r', encoding='utf-8') as f:\r\n                data = json.load(f)\r\n            return data\r\n        except Exception as e:\r\n            print(f\"‚ùå L·ªói khi ƒë·ªçc file {filepath}: {e}\")\r\n            return {}\r\n    \r\n    def extract_conversations(self, data: Dict[Any, Any]) -> List[Dict[str, Any]]:\r\n        \"\"\"\r\n        Tr√≠ch xu·∫•t v√† t·ªï ch·ª©c d·ªØ li·ªáu conversation\r\n        \"\"\"\r\n        if 'data' not in data:\r\n            print(\"‚ùå Kh√¥ng t√¨m th·∫•y key 'data' trong JSON\")\r\n            return []\r\n        \r\n        conversations = []\r\n        current_conversation = []\r\n        \r\n        for item in data['data']:\r\n            character = item.get('character', '')\r\n            content = item.get('content', '')\r\n            \r\n"
            },
            {
              "count": 4,
              "added": true,
              "removed": false,
              "value": "            # B·ªè qua n·∫øu content r·ªóng\r\n            if not content.strip():\r\n                continue\r\n                \r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "            if character == 'BOT_RESPONSE_CONVERSATION':\r\n"
            },
            {
              "count": 7,
              "added": false,
              "removed": true,
              "value": "                if current_conversation:\r\n                    # K·∫øt th√∫c conversation hi·ªán t·∫°i\r\n                    conversations.append({\r\n                        'conversation': current_conversation.copy(),\r\n                        'next_fast_response': '',\r\n                        'next_bot_response': content\r\n                    })\r\n"
            },
            {
              "count": 7,
              "added": true,
              "removed": false,
              "value": "                current_conversation.append({\"role\": \"assistant\", \"content\": content})\r\n                # L∆∞u conversation hi·ªán t·∫°i\r\n                conversations.append({\r\n                    'conversation': current_conversation.copy(),\r\n                    'next_fast_response': '',\r\n                    'next_bot_response': ''\r\n                })\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "                \r\n"
            },
            {
              "count": 3,
              "added": false,
              "removed": true,
              "value": "                # B·∫Øt ƒë·∫ßu conversation m·ªõi\r\n                current_conversation = [{\"role\": \"assistant\", \"content\": content}]\r\n                \r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "            elif character == 'USER':\r\n"
            },
            {
              "count": 7,
              "added": false,
              "removed": true,
              "value": "                if content != '-':  # B·ªè qua user input r·ªóng\r\n                    current_conversation.append({\"role\": \"user\", \"content\": content})\r\n                    \r\n            elif character == 'FAST_RESPONSE':\r\n                # C·∫≠p nh·∫≠t fast response cho conversation g·∫ßn nh·∫•t\r\n                if conversations:\r\n                    conversations[-1]['next_fast_response'] = content\r\n"
            },
            {
              "count": 7,
              "added": true,
              "removed": false,
              "value": "                current_conversation.append({\"role\": \"user\", \"content\": content})\r\n                # L∆∞u conversation hi·ªán t·∫°i sau khi c√≥ user input\r\n                conversations.append({\r\n                    'conversation': current_conversation.copy(),\r\n                    'next_fast_response': '',\r\n                    'next_bot_response': ''\r\n                })\r\n"
            },
            {
              "count": 80,
              "added": false,
              "removed": false,
              "value": "        \r\n        return conversations\r\n    \r\n    def format_conversation_column(self, conversation: List[Dict[str, str]]) -> str:\r\n        \"\"\"\r\n        Format conversation th√†nh string JSON\r\n        \"\"\"\r\n        return json.dumps(conversation, ensure_ascii=False)\r\n    \r\n    def process_to_dataframe(self, conversations: List[Dict[str, Any]]) -> pd.DataFrame:\r\n        \"\"\"\r\n        Chuy·ªÉn ƒë·ªïi conversations th√†nh DataFrame\r\n        \"\"\"\r\n        processed_data = []\r\n        \r\n        for conv in conversations:\r\n            processed_data.append({\r\n                'BOT_RESPONSE_CONVERSATION_with_USER': self.format_conversation_column(conv['conversation']),\r\n                'FAST_RESPONSE_next': conv['next_fast_response'],\r\n                'BOT_RESPONSE_CONVERSATION_next': conv['next_bot_response']\r\n            })\r\n        \r\n        return pd.DataFrame(processed_data)\r\n    \r\n    def process_file(self, input_filepath: str, output_filepath: str) -> bool:\r\n        \"\"\"\r\n        X·ª≠ l√Ω file JSON v√† xu·∫•t ra Excel\r\n        \"\"\"\r\n        try:\r\n            # Load d·ªØ li·ªáu\r\n            data = self.load_json_data(input_filepath)\r\n            if not data:\r\n                return False\r\n            \r\n            # Tr√≠ch xu·∫•t conversations\r\n            conversations = self.extract_conversations(data)\r\n            if not conversations:\r\n                print(\"‚ùå Kh√¥ng t√¨m th·∫•y conversation n√†o\")\r\n                return False\r\n            \r\n            # T·∫°o DataFrame\r\n            df = self.process_to_dataframe(conversations)\r\n            \r\n            # Xu·∫•t ra Excel\r\n            df.to_excel(output_filepath, index=False, engine='openpyxl')\r\n            print(f\"‚úÖ ƒê√£ xu·∫•t d·ªØ li·ªáu ra: {output_filepath}\")\r\n            print(f\"üìä S·ªë l∆∞·ª£ng conversations: {len(conversations)}\")\r\n            \r\n            return True\r\n            \r\n        except Exception as e:\r\n            print(f\"‚ùå L·ªói khi x·ª≠ l√Ω file: {e}\")\r\n            return False\r\n\r\ndef process_all_input_files():\r\n    \"\"\"\r\n    X·ª≠ l√Ω t·∫•t c·∫£ file trong folder input\r\n    \"\"\"\r\n    processor = ConversationProcessor()\r\n    \r\n    if not os.path.exists('input'):\r\n        print(\"‚ùå Folder 'input' kh√¥ng t·ªìn t·∫°i\")\r\n        return\r\n    \r\n    # T·∫°o folder output n·∫øu ch∆∞a c√≥\r\n    if not os.path.exists('output'):\r\n        os.makedirs('output')\r\n    \r\n    # X·ª≠ l√Ω t·ª´ng file JSON trong folder input\r\n    for filename in os.listdir('input'):\r\n        if filename.endswith('.json'):\r\n            input_path = os.path.join('input', filename)\r\n            output_filename = filename.replace('.json', '_processed.xlsx')\r\n            output_path = os.path.join('output', output_filename)\r\n            \r\n            print(f\"\\nüîÑ ƒêang x·ª≠ l√Ω: {filename}\")\r\n            processor.process_file(input_path, output_path)\r\n\r\nif __name__ == \"__main__\":\r\n    process_all_input_files()\r\n"
            }
          ]
        }
      ],
      "selection": null,
      "timestamp": "2025-06-24T04:31:41.821Z",
      "prompt": "def extract_conversations(self, data: Dict[Any, Any]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Tr√≠ch xu·∫•t v√† t·ªï ch·ª©c d·ªØ li·ªáu conversation\n    \"\"\"\n    if 'data' not in data:\n        print(\"‚ùå Kh√¥ng t√¨m th·∫•y key 'data' trong JSON\")\n        return []\n    \n    conversations = []\n    current_conversation = []\n    \n    for item in data['data']:\n        character = item.get('character', '')\n        content = item.get('content', '')\n        \n        # B·ªè qua n·∫øu content r·ªóng\n        if not content.strip():\n            continue\n            \n        if character == 'BOT_RESPONSE_CONVERSATION':\n            current_conversation.append({\"role\": \"assistant\", \"content\": content})\n            # L∆∞u conversation hi·ªán t·∫°i\n            conversations.append({\n                'conversation': current_conversation.copy(),\n                'next_fast_response': '',\n                'next_bot_response': ''\n            })\n            \n        elif character == 'USER':\n            current_conversation.append({\"role\": \"user\", \"content\": content})\n            # L∆∞u conversation hi·ªán t·∫°i sau khi c√≥ user input\n            conversations.append({\n                'conversation': current_conversation.copy(),\n                'next_fast_response': '',\n                'next_bot_response': ''\n            })\n    \n    return conversations",
      "parent_inline_chat_id": null
    },
    {
      "inline_chat_id": "79bff3ea-3f1b-498c-9a24-848164384869",
      "file_diff": [
        {
          "file_path": "7_eval_compareFastResponse/processed.py",
          "diffs": [
            {
              "count": 34,
              "added": false,
              "removed": false,
              "value": "import json\r\nimport pandas as pd\r\nimport os\r\nfrom typing import List, Dict, Any\r\n\r\nclass ConversationProcessor:\r\n    def __init__(self):\r\n        pass\r\n    \r\n    def load_json_data(self, filepath: str) -> Dict[Any, Any]:\r\n        \"\"\"\r\n        Load d·ªØ li·ªáu t·ª´ file JSON\r\n        \"\"\"\r\n        try:\r\n            with open(filepath, 'r', encoding='utf-8') as f:\r\n                data = json.load(f)\r\n            return data\r\n        except Exception as e:\r\n            print(f\"‚ùå L·ªói khi ƒë·ªçc file {filepath}: {e}\")\r\n            return {}\r\n    \r\n    def extract_conversations(self, data: Dict[Any, Any]) -> List[Dict[str, Any]]:\r\n        \"\"\"\r\n        Tr√≠ch xu·∫•t v√† t·ªï ch·ª©c d·ªØ li·ªáu conversation\r\n        \"\"\"\r\n        if 'data' not in data:\r\n            print(\"‚ùå Kh√¥ng t√¨m th·∫•y key 'data' trong JSON\")\r\n            return []\r\n        \r\n        conversations = []\r\n        current_conversation = []\r\n        \r\n        for item in data['data']:\r\n            character = item.get('character', '')\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "            content = item.get('content', '')\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "            content = item.get('content', '').strip()\r\n"
            },
            {
              "count": 2,
              "added": false,
              "removed": false,
              "value": "            \r\n            # B·ªè qua n·∫øu content r·ªóng\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "            if not content.strip():\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "            if not content:\r\n"
            },
            {
              "count": 4,
              "added": false,
              "removed": false,
              "value": "                continue\r\n                \r\n            if character == 'BOT_RESPONSE_CONVERSATION':\r\n                current_conversation.append({\"role\": \"assistant\", \"content\": content})\r\n"
            },
            {
              "count": 6,
              "added": false,
              "removed": true,
              "value": "                # L∆∞u conversation hi·ªán t·∫°i\r\n                conversations.append({\r\n                    'conversation': current_conversation.copy(),\r\n                    'next_fast_response': '',\r\n                    'next_bot_response': ''\r\n                })\r\n"
            },
            {
              "count": 3,
              "added": false,
              "removed": false,
              "value": "                \r\n            elif character == 'USER':\r\n                current_conversation.append({\"role\": \"user\", \"content\": content})\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "                # L∆∞u conversation hi·ªán t·∫°i sau khi c√≥ user input\r\n"
            },
            {
              "count": 2,
              "added": true,
              "removed": false,
              "value": "                \r\n                # Khi g·∫∑p USER, t·∫°o m·ªôt conversation entry\r\n"
            },
            {
              "count": 6,
              "added": false,
              "removed": false,
              "value": "                conversations.append({\r\n                    'conversation': current_conversation.copy(),\r\n                    'next_fast_response': '',\r\n                    'next_bot_response': ''\r\n                })\r\n        \r\n"
            },
            {
              "count": 15,
              "added": true,
              "removed": false,
              "value": "        # C·∫≠p nh·∫≠t next_fast_response v√† next_bot_response\r\n        for i, item in enumerate(data['data']):\r\n            if item.get('character') == 'FAST_RESPONSE':\r\n                # T√¨m conversation t∆∞∆°ng ·ª©ng ƒë·ªÉ c·∫≠p nh·∫≠t\r\n                for conv in conversations:\r\n                    if not conv['next_fast_response']:  # C·∫≠p nh·∫≠t cho conversation ch∆∞a c√≥ fast_response\r\n                        conv['next_fast_response'] = item.get('content', '')\r\n                        break\r\n            elif item.get('character') == 'BOT_RESPONSE_CONVERSATION' and item.get('content', '').strip():\r\n                # T√¨m conversation t∆∞∆°ng ·ª©ng ƒë·ªÉ c·∫≠p nh·∫≠t next_bot_response\r\n                for conv in conversations:\r\n                    if not conv['next_bot_response']:  # C·∫≠p nh·∫≠t cho conversation ch∆∞a c√≥ bot_response\r\n                        conv['next_bot_response'] = item.get('content', '')\r\n                        break\r\n        \r\n"
            },
            {
              "count": 79,
              "added": false,
              "removed": false,
              "value": "        return conversations\r\n    \r\n    def format_conversation_column(self, conversation: List[Dict[str, str]]) -> str:\r\n        \"\"\"\r\n        Format conversation th√†nh string JSON\r\n        \"\"\"\r\n        return json.dumps(conversation, ensure_ascii=False)\r\n    \r\n    def process_to_dataframe(self, conversations: List[Dict[str, Any]]) -> pd.DataFrame:\r\n        \"\"\"\r\n        Chuy·ªÉn ƒë·ªïi conversations th√†nh DataFrame\r\n        \"\"\"\r\n        processed_data = []\r\n        \r\n        for conv in conversations:\r\n            processed_data.append({\r\n                'BOT_RESPONSE_CONVERSATION_with_USER': self.format_conversation_column(conv['conversation']),\r\n                'FAST_RESPONSE_next': conv['next_fast_response'],\r\n                'BOT_RESPONSE_CONVERSATION_next': conv['next_bot_response']\r\n            })\r\n        \r\n        return pd.DataFrame(processed_data)\r\n    \r\n    def process_file(self, input_filepath: str, output_filepath: str) -> bool:\r\n        \"\"\"\r\n        X·ª≠ l√Ω file JSON v√† xu·∫•t ra Excel\r\n        \"\"\"\r\n        try:\r\n            # Load d·ªØ li·ªáu\r\n            data = self.load_json_data(input_filepath)\r\n            if not data:\r\n                return False\r\n            \r\n            # Tr√≠ch xu·∫•t conversations\r\n            conversations = self.extract_conversations(data)\r\n            if not conversations:\r\n                print(\"‚ùå Kh√¥ng t√¨m th·∫•y conversation n√†o\")\r\n                return False\r\n            \r\n            # T·∫°o DataFrame\r\n            df = self.process_to_dataframe(conversations)\r\n            \r\n            # Xu·∫•t ra Excel\r\n            df.to_excel(output_filepath, index=False, engine='openpyxl')\r\n            print(f\"‚úÖ ƒê√£ xu·∫•t d·ªØ li·ªáu ra: {output_filepath}\")\r\n            print(f\"üìä S·ªë l∆∞·ª£ng conversations: {len(conversations)}\")\r\n            \r\n            return True\r\n            \r\n        except Exception as e:\r\n            print(f\"‚ùå L·ªói khi x·ª≠ l√Ω file: {e}\")\r\n            return False\r\n\r\ndef process_all_input_files():\r\n    \"\"\"\r\n    X·ª≠ l√Ω t·∫•t c·∫£ file trong folder input\r\n    \"\"\"\r\n    processor = ConversationProcessor()\r\n    \r\n    if not os.path.exists('input'):\r\n        print(\"‚ùå Folder 'input' kh√¥ng t·ªìn t·∫°i\")\r\n        return\r\n    \r\n    # T·∫°o folder output n·∫øu ch∆∞a c√≥\r\n    if not os.path.exists('output'):\r\n        os.makedirs('output')\r\n    \r\n    # X·ª≠ l√Ω t·ª´ng file JSON trong folder input\r\n    for filename in os.listdir('input'):\r\n        if filename.endswith('.json'):\r\n            input_path = os.path.join('input', filename)\r\n            output_filename = filename.replace('.json', '_processed.xlsx')\r\n            output_path = os.path.join('output', output_filename)\r\n            \r\n            print(f\"\\nüîÑ ƒêang x·ª≠ l√Ω: {filename}\")\r\n            processor.process_file(input_path, output_path)\r\n\r\nif __name__ == \"__main__\":\r\n    process_all_input_files()\r\n"
            }
          ]
        }
      ],
      "selection": null,
      "timestamp": "2025-06-24T04:34:00.125Z",
      "prompt": "def extract_conversations(self, data: Dict[Any, Any]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Tr√≠ch xu·∫•t v√† t·ªï ch·ª©c d·ªØ li·ªáu conversation\n    \"\"\"\n    if 'data' not in data:\n        print(\"‚ùå Kh√¥ng t√¨m th·∫•y key 'data' trong JSON\")\n        return []\n    \n    conversations = []\n    current_conversation = []\n    \n    for item in data['data']:\n        character = item.get('character', '')\n        content = item.get('content', '').strip()\n        \n        # B·ªè qua n·∫øu content r·ªóng\n        if not content:\n            continue\n            \n        if character == 'BOT_RESPONSE_CONVERSATION':\n            current_conversation.append({\"role\": \"assistant\", \"content\": content})\n            \n        elif character == 'USER':\n            current_conversation.append({\"role\": \"user\", \"content\": content})\n            \n            # Khi g·∫∑p USER, t·∫°o m·ªôt conversation entry\n            conversations.append({\n                'conversation': current_conversation.copy(),\n                'next_fast_response': '',\n                'next_bot_response': ''\n            })\n    \n    # C·∫≠p nh·∫≠t next_fast_response v√† next_bot_response\n    for i, item in enumerate(data['data']):\n        if item.get('character') == 'FAST_RESPONSE':\n            # T√¨m conversation t∆∞∆°ng ·ª©ng ƒë·ªÉ c·∫≠p nh·∫≠t\n            for conv in conversations:\n                if not conv['next_fast_response']:  # C·∫≠p nh·∫≠t cho conversation ch∆∞a c√≥ fast_response\n                    conv['next_fast_response'] = item.get('content', '')\n                    break\n        elif item.get('character') == 'BOT_RESPONSE_CONVERSATION' and item.get('content', '').strip():\n            # T√¨m conversation t∆∞∆°ng ·ª©ng ƒë·ªÉ c·∫≠p nh·∫≠t next_bot_response\n            for conv in conversations:\n                if not conv['next_bot_response']:  # C·∫≠p nh·∫≠t cho conversation ch∆∞a c√≥ bot_response\n                    conv['next_bot_response'] = item.get('content', '')\n                    break\n    \n    return conversations\n",
      "parent_inline_chat_id": null
    },
    {
      "inline_chat_id": "a6eb339b-8a9f-4144-9cbd-b321e5c54751",
      "file_diff": [
        {
          "file_path": "7_eval_compareFastResponse/run_eval_api_fast_response.py",
          "diffs": [
            {
              "count": 16,
              "added": false,
              "removed": false,
              "value": "import pandas as pd\r\nimport requests\r\nimport json\r\nimport time\r\nfrom typing import Dict, List, Any\r\nimport os\r\nimport time\r\n\r\nclass FastResponseEvaluator:\r\n    def __init__(self):\r\n        self.api_url = \"http://103.253.20.30:8990/fast_response/generate\"\r\n        self.headers = {\r\n            'accept': 'application/json',\r\n            'Content-Type': 'application/json'\r\n        }\r\n    \r\n"
            },
            {
              "count": 17,
              "added": false,
              "removed": true,
              "value": "def call_fast_response_api(self, conversations: List[Dict[str, str]]) -> tuple:\r\n    \"\"\"\r\n    G·ªçi API fast response v√† ƒëo th·ªùi gian\r\n    \"\"\"\r\n    start_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n    \r\n    payload = {\r\n        \"conversations\": conversations,\r\n        \"system_prompt\": \"You are QuickReact: detect the emotion in the latest message and reply instantly in its same language (English or Vietnamese) using 1-8 words (‚â§60 chars), keep it short enough with a friendly informal tone that mirrors and empathizes with that feeling (sad ‚Üí soothe, happy ‚Üí cheer, worried ‚Üí reassure; emojis/!/? welcome); output only that text‚Äînever answer the question, just buy time until the main reply arrives.\",\r\n        \"model_name\": \"Qwen/Qwen3-4B\",\r\n        \"temperature\": 0.8,\r\n        \"top_p\": 1\r\n    }\r\n    \r\n    try:\r\n        response = requests.post(self.api_url, headers=self.headers, json=payload, timeout=30)\r\n        response.raise_for_status()\r\n"
            },
            {
              "count": 5,
              "added": true,
              "removed": false,
              "value": "    def call_fast_response_api(self, conversations: List[Dict[str, str]]) -> tuple:\r\n        \"\"\"\r\n        G·ªçi API fast response v√† ƒëo th·ªùi gian\r\n        \"\"\"\r\n        start_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "        \r\n"
            },
            {
              "count": 2,
              "added": false,
              "removed": true,
              "value": "        end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n        response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY (milliseconds)\r\n"
            },
            {
              "count": 7,
              "added": true,
              "removed": false,
              "value": "        payload = {\r\n            \"conversations\": conversations,\r\n            \"system_prompt\": \"You are QuickReact: detect the emotion in the latest message and reply instantly in its same language (English or Vietnamese) using 1-8 words (‚â§60 chars), keep it short enough with a friendly informal tone that mirrors and empathizes with that feeling (sad ‚Üí soothe, happy ‚Üí cheer, worried ‚Üí reassure; emojis/!/? welcome); output only that text‚Äînever answer the question, just buy time until the main reply arrives.\",\r\n            \"model_name\": \"Qwen/Qwen3-4B\",\r\n            \"temperature\": 0.8,\r\n            \"top_p\": 1\r\n        }\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "        \r\n"
            },
            {
              "count": 20,
              "added": false,
              "removed": true,
              "value": "        result = response.json()\r\n        if isinstance(result, dict):\r\n            return result.get('response', result.get('content', str(result))), response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n        return str(result), response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n        \r\n    except requests.exceptions.Timeout:\r\n        end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n        response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n        print(\"‚è∞ API timeout\")\r\n        return \"API_TIMEOUT\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n    except requests.exceptions.RequestException as e:\r\n        end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n        response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n        print(f\"‚ùå API Error: {e}\")\r\n        return f\"API_ERROR: {str(e)}\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n    except Exception as e:\r\n        end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n        response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n        print(f\"‚ùå Unexpected error: {e}\")\r\n        return f\"ERROR: {str(e)}\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n"
            },
            {
              "count": 27,
              "added": true,
              "removed": false,
              "value": "        try:\r\n            response = requests.post(self.api_url, headers=self.headers, json=payload, timeout=30)\r\n            response.raise_for_status()\r\n            \r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY (milliseconds)\r\n            \r\n            result = response.json()\r\n            if isinstance(result, dict):\r\n                return result.get('response', result.get('content', str(result))), response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n            return str(result), response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n            \r\n        except requests.exceptions.Timeout:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(\"‚è∞ API timeout\")\r\n            return \"API_TIMEOUT\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n        except requests.exceptions.RequestException as e:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(f\"‚ùå API Error: {e}\")\r\n            return f\"API_ERROR: {str(e)}\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n        except Exception as e:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(f\"‚ùå Unexpected error: {e}\")\r\n            return f\"ERROR: {str(e)}\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n"
            },
            {
              "count": 78,
              "added": false,
              "removed": false,
              "value": "    \r\n    def parse_conversation_string(self, conv_str: str) -> List[Dict[str, str]]:\r\n        \"\"\"\r\n        Parse conversation string th√†nh list\r\n        \"\"\"\r\n        try:\r\n            return json.loads(conv_str)\r\n        except json.JSONDecodeError:\r\n            print(f\"‚ùå L·ªói parse conversation: {conv_str[:100]}...\")\r\n            return []\r\n    \r\n    def evaluate_excel_file(self, input_filepath: str, output_filepath: str) -> bool:\r\n        \"\"\"\r\n        ƒê√°nh gi√° file Excel v√† t·∫°o file output\r\n        \"\"\"\r\n        try:\r\n            # ƒê·ªçc file Excel\r\n            df = pd.read_excel(input_filepath)\r\n            print(f\"üìä ƒê√£ ƒë·ªçc {len(df)} rows t·ª´ {input_filepath}\")\r\n            \r\n            # Th√™m c·ªôt generated_ai\r\n            df['generated_ai'] = ''\r\n            \r\n            # X·ª≠ l√Ω t·ª´ng row\r\n            for index, row in df.iterrows():\r\n                print(f\"üîÑ ƒêang x·ª≠ l√Ω row {index + 1}/{len(df)}\")\r\n                \r\n                conv_str = row['BOT_RESPONSE_CONVERSATION_with_USER']\r\n                conversations = self.parse_conversation_string(conv_str)\r\n                \r\n                if conversations:\r\n                    # G·ªçi API\r\n                    generated_response = self.call_fast_response_api(conversations)\r\n                    df.at[index, 'generated_ai'] = generated_response\r\n                    \r\n                    # Delay ƒë·ªÉ tr√°nh spam API\r\n                    time.sleep(0.5)\r\n                else:\r\n                    df.at[index, 'generated_ai'] = \"PARSE_ERROR\"\r\n                \r\n                # In progress m·ªói 10 rows\r\n                if (index + 1) % 10 == 0:\r\n                    print(f\"‚úÖ ƒê√£ ho√†n th√†nh {index + 1}/{len(df)} rows\")\r\n            \r\n            # L∆∞u file output\r\n            df.to_excel(output_filepath, index=False, engine='openpyxl')\r\n            print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ ra: {output_filepath}\")\r\n            \r\n            return True\r\n            \r\n        except Exception as e:\r\n            print(f\"‚ùå L·ªói khi ƒë√°nh gi√° file: {e}\")\r\n            return False\r\n    \r\n    def evaluate_all_processed_files(self):\r\n        \"\"\"\r\n        ƒê√°nh gi√° t·∫•t c·∫£ file processed trong folder output\r\n        \"\"\"\r\n        if not os.path.exists('output'):\r\n            print(\"‚ùå Folder 'output' kh√¥ng t·ªìn t·∫°i\")\r\n            return\r\n        \r\n        # T·∫°o folder eval n·∫øu ch∆∞a c√≥\r\n        if not os.path.exists('eval'):\r\n            os.makedirs('eval')\r\n        \r\n        for filename in os.listdir('output'):\r\n            if filename.endswith('_processed.xlsx'):\r\n                input_path = os.path.join('output', filename)\r\n                output_filename = filename.replace('_processed.xlsx', '_output_eval.xlsx')\r\n                output_path = os.path.join('eval', output_filename)\r\n                \r\n                print(f\"\\nüîÑ ƒêang ƒë√°nh gi√°: {filename}\")\r\n                self.evaluate_excel_file(input_path, output_path)\r\n\r\nif __name__ == \"__main__\":\r\n    evaluator = FastResponseEvaluator()\r\n    evaluator.evaluate_all_processed_files()\r\n"
            }
          ]
        }
      ],
      "selection": null,
      "timestamp": "2025-06-24T05:03:44.815Z",
      "prompt": "# Th√™m c·ªôt response_time\ndf['generated_ai'] = ''\ndf['response_time'] = ''  # ‚Üê TH√äM D√íNG N√ÄY\n\n# X·ª≠ l√Ω t·ª´ng row\nfor index, row in df.iterrows():\n    print(f\"üîÑ ƒêang x·ª≠ l√Ω row {index + 1}/{len(df)}\")\n    \n    conv_str = row['BOT_RESPONSE_CONVERSATION_with_USER']\n    conversations = self.parse_conversation_string(conv_str)\n    \n    if conversations:\n        # G·ªçi API\n        generated_response, response_time = self.call_fast_response_api(conversations)  # ‚Üê S·ª¨A D√íNG N√ÄY\n        df.at[index, 'generated_ai'] = generated_response\n        df.at[index, 'response_time'] = response_time  # ‚Üê TH√äM D√íNG N√ÄY\n        \n        # Delay ƒë·ªÉ tr√°nh spam API\n        time.sleep(0.5)\n    else:\n        df.at[index, 'generated_ai'] = \"PARSE_ERROR\"\n        df.at[index, 'response_time'] = 0  # ‚Üê TH√äM D√íNG N√ÄY\n",
      "parent_inline_chat_id": null
    },
    {
      "inline_chat_id": "d9a5d837-2160-4556-baf5-de8a61d8ee64",
      "file_diff": [
        {
          "file_path": "7_eval_compareFastResponse/run_eval_api_fast_response.py",
          "diffs": [
            {
              "count": 77,
              "added": false,
              "removed": false,
              "value": "import pandas as pd\r\nimport requests\r\nimport json\r\nimport time\r\nfrom typing import Dict, List, Any\r\nimport os\r\nimport time\r\n\r\nclass FastResponseEvaluator:\r\n    def __init__(self):\r\n        self.api_url = \"http://103.253.20.30:8990/fast_response/generate\"\r\n        self.headers = {\r\n            'accept': 'application/json',\r\n            'Content-Type': 'application/json'\r\n        }\r\n    \r\n    def call_fast_response_api(self, conversations: List[Dict[str, str]]) -> tuple:\r\n        \"\"\"\r\n        G·ªçi API fast response v√† ƒëo th·ªùi gian\r\n        \"\"\"\r\n        start_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n        \r\n        payload = {\r\n            \"conversations\": conversations,\r\n            \"system_prompt\": \"You are QuickReact: detect the emotion in the latest message and reply instantly in its same language (English or Vietnamese) using 1-8 words (‚â§60 chars), keep it short enough with a friendly informal tone that mirrors and empathizes with that feeling (sad ‚Üí soothe, happy ‚Üí cheer, worried ‚Üí reassure; emojis/!/? welcome); output only that text‚Äînever answer the question, just buy time until the main reply arrives.\",\r\n            \"model_name\": \"Qwen/Qwen3-4B\",\r\n            \"temperature\": 0.8,\r\n            \"top_p\": 1\r\n        }\r\n        \r\n        try:\r\n            response = requests.post(self.api_url, headers=self.headers, json=payload, timeout=30)\r\n            response.raise_for_status()\r\n            \r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY (milliseconds)\r\n            \r\n            result = response.json()\r\n            if isinstance(result, dict):\r\n                return result.get('response', result.get('content', str(result))), response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n            return str(result), response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n            \r\n        except requests.exceptions.Timeout:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(\"‚è∞ API timeout\")\r\n            return \"API_TIMEOUT\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n        except requests.exceptions.RequestException as e:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(f\"‚ùå API Error: {e}\")\r\n            return f\"API_ERROR: {str(e)}\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n        except Exception as e:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(f\"‚ùå Unexpected error: {e}\")\r\n            return f\"ERROR: {str(e)}\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n    \r\n    def parse_conversation_string(self, conv_str: str) -> List[Dict[str, str]]:\r\n        \"\"\"\r\n        Parse conversation string th√†nh list\r\n        \"\"\"\r\n        try:\r\n            return json.loads(conv_str)\r\n        except json.JSONDecodeError:\r\n            print(f\"‚ùå L·ªói parse conversation: {conv_str[:100]}...\")\r\n            return []\r\n    \r\n    def evaluate_excel_file(self, input_filepath: str, output_filepath: str) -> bool:\r\n        \"\"\"\r\n        ƒê√°nh gi√° file Excel v√† t·∫°o file output\r\n        \"\"\"\r\n        try:\r\n            # ƒê·ªçc file Excel\r\n            df = pd.read_excel(input_filepath)\r\n            print(f\"üìä ƒê√£ ƒë·ªçc {len(df)} rows t·ª´ {input_filepath}\")\r\n            \r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "            # Th√™m c·ªôt generated_ai v√† response_time\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "            # Th√™m c·ªôt generated_ai\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "            df['generated_ai'] = ''\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "            df['response_time'] = ''  # ‚Üê TH√äM D√íNG N√ÄY\r\n"
            },
            {
              "count": 10,
              "added": false,
              "removed": false,
              "value": "            \r\n            # X·ª≠ l√Ω t·ª´ng row\r\n            for index, row in df.iterrows():\r\n                print(f\"üîÑ ƒêang x·ª≠ l√Ω row {index + 1}/{len(df)}\")\r\n                \r\n                conv_str = row['BOT_RESPONSE_CONVERSATION_with_USER']\r\n                conversations = self.parse_conversation_string(conv_str)\r\n                \r\n                if conversations:\r\n                    # G·ªçi API\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "                    generated_response, response_time = self.call_fast_response_api(conversations)  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "                    generated_response = self.call_fast_response_api(conversations)\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "                    df.at[index, 'generated_ai'] = generated_response\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "                    df.at[index, 'response_time'] = response_time  # ‚Üê TH√äM D√íNG N√ÄY\r\n"
            },
            {
              "count": 5,
              "added": false,
              "removed": false,
              "value": "                    \r\n                    # Delay ƒë·ªÉ tr√°nh spam API\r\n                    time.sleep(0.5)\r\n                else:\r\n                    df.at[index, 'generated_ai'] = \"PARSE_ERROR\"\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "                    df.at[index, 'response_time'] = 0  # ‚Üê TH√äM D√íNG N√ÄY\r\n"
            },
            {
              "count": 39,
              "added": false,
              "removed": false,
              "value": "                \r\n                # In progress m·ªói 10 rows\r\n                if (index + 1) % 10 == 0:\r\n                    print(f\"‚úÖ ƒê√£ ho√†n th√†nh {index + 1}/{len(df)} rows\")\r\n            \r\n            # L∆∞u file output\r\n            df.to_excel(output_filepath, index=False, engine='openpyxl')\r\n            print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ ra: {output_filepath}\")\r\n            \r\n            return True\r\n            \r\n        except Exception as e:\r\n            print(f\"‚ùå L·ªói khi ƒë√°nh gi√° file: {e}\")\r\n            return False\r\n    \r\n    def evaluate_all_processed_files(self):\r\n        \"\"\"\r\n        ƒê√°nh gi√° t·∫•t c·∫£ file processed trong folder output\r\n        \"\"\"\r\n        if not os.path.exists('output'):\r\n            print(\"‚ùå Folder 'output' kh√¥ng t·ªìn t·∫°i\")\r\n            return\r\n        \r\n        # T·∫°o folder eval n·∫øu ch∆∞a c√≥\r\n        if not os.path.exists('eval'):\r\n            os.makedirs('eval')\r\n        \r\n        for filename in os.listdir('output'):\r\n            if filename.endswith('_processed.xlsx'):\r\n                input_path = os.path.join('output', filename)\r\n                output_filename = filename.replace('_processed.xlsx', '_output_eval.xlsx')\r\n                output_path = os.path.join('eval', output_filename)\r\n                \r\n                print(f\"\\nüîÑ ƒêang ƒë√°nh gi√°: {filename}\")\r\n                self.evaluate_excel_file(input_path, output_path)\r\n\r\nif __name__ == \"__main__\":\r\n    evaluator = FastResponseEvaluator()\r\n    evaluator.evaluate_all_processed_files()\r\n"
            }
          ]
        }
      ],
      "selection": null,
      "timestamp": "2025-06-24T05:04:14.600Z",
      "prompt": "# Th√™m c·ªôt response_time\ndf['generated_ai'] = ''\ndf['response_time'] = ''  # ‚Üê TH√äM D√íNG N√ÄY\n\n# X·ª≠ l√Ω t·ª´ng row\nfor index, row in df.iterrows():\n    print(f\"üîÑ ƒêang x·ª≠ l√Ω row {index + 1}/{len(df)}\")\n    \n    conv_str = row['BOT_RESPONSE_CONVERSATION_with_USER']\n    conversations = self.parse_conversation_string(conv_str)\n    \n    if conversations:\n        # G·ªçi API\n        generated_response, response_time = self.call_fast_response_api(conversations)  # ‚Üê S·ª¨A D√íNG N√ÄY\n        df.at[index, 'generated_ai'] = generated_response\n        df.at[index, 'response_time'] = response_time  # ‚Üê TH√äM D√íNG N√ÄY\n        \n        # Delay ƒë·ªÉ tr√°nh spam API\n        time.sleep(0.5)\n    else:\n        df.at[index, 'generated_ai'] = \"PARSE_ERROR\"\n        df.at[index, 'response_time'] = 0  # ‚Üê TH√äM D√íNG N√ÄY",
      "parent_inline_chat_id": null
    },
    {
      "inline_chat_id": "08ed8a0d-e3af-4f96-94ac-c8ea2c7ebb21",
      "file_diff": [
        {
          "file_path": "7_eval_compareFastResponse/run_eval_api_fast_response.py",
          "diffs": [
            {
              "count": 77,
              "added": false,
              "removed": false,
              "value": "import pandas as pd\r\nimport requests\r\nimport json\r\nimport time\r\nfrom typing import Dict, List, Any\r\nimport os\r\nimport time\r\n\r\nclass FastResponseEvaluator:\r\n    def __init__(self):\r\n        self.api_url = \"http://103.253.20.30:8990/fast_response/generate\"\r\n        self.headers = {\r\n            'accept': 'application/json',\r\n            'Content-Type': 'application/json'\r\n        }\r\n    \r\n    def call_fast_response_api(self, conversations: List[Dict[str, str]]) -> tuple:\r\n        \"\"\"\r\n        G·ªçi API fast response v√† ƒëo th·ªùi gian\r\n        \"\"\"\r\n        start_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n        \r\n        payload = {\r\n            \"conversations\": conversations,\r\n            \"system_prompt\": \"You are QuickReact: detect the emotion in the latest message and reply instantly in its same language (English or Vietnamese) using 1-8 words (‚â§60 chars), keep it short enough with a friendly informal tone that mirrors and empathizes with that feeling (sad ‚Üí soothe, happy ‚Üí cheer, worried ‚Üí reassure; emojis/!/? welcome); output only that text‚Äînever answer the question, just buy time until the main reply arrives.\",\r\n            \"model_name\": \"Qwen/Qwen3-4B\",\r\n            \"temperature\": 0.8,\r\n            \"top_p\": 1\r\n        }\r\n        \r\n        try:\r\n            response = requests.post(self.api_url, headers=self.headers, json=payload, timeout=30)\r\n            response.raise_for_status()\r\n            \r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY (milliseconds)\r\n            \r\n            result = response.json()\r\n            if isinstance(result, dict):\r\n                return result.get('response', result.get('content', str(result))), response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n            return str(result), response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n            \r\n        except requests.exceptions.Timeout:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(\"‚è∞ API timeout\")\r\n            return \"API_TIMEOUT\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n        except requests.exceptions.RequestException as e:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(f\"‚ùå API Error: {e}\")\r\n            return f\"API_ERROR: {str(e)}\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n        except Exception as e:\r\n            end_time = time.time()  # ‚Üê TH√äM D√íNG N√ÄY\r\n            response_time = round((end_time - start_time) * 1000, 2)  # ‚Üê TH√äM D√íNG N√ÄY\r\n            print(f\"‚ùå Unexpected error: {e}\")\r\n            return f\"ERROR: {str(e)}\", response_time  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n    \r\n    def parse_conversation_string(self, conv_str: str) -> List[Dict[str, str]]:\r\n        \"\"\"\r\n        Parse conversation string th√†nh list\r\n        \"\"\"\r\n        try:\r\n            return json.loads(conv_str)\r\n        except json.JSONDecodeError:\r\n            print(f\"‚ùå L·ªói parse conversation: {conv_str[:100]}...\")\r\n            return []\r\n    \r\n    def evaluate_excel_file(self, input_filepath: str, output_filepath: str) -> bool:\r\n        \"\"\"\r\n        ƒê√°nh gi√° file Excel v√† t·∫°o file output\r\n        \"\"\"\r\n        try:\r\n            # ƒê·ªçc file Excel\r\n            df = pd.read_excel(input_filepath)\r\n            print(f\"üìä ƒê√£ ƒë·ªçc {len(df)} rows t·ª´ {input_filepath}\")\r\n            \r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "            # Th√™m c·ªôt generated_ai\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "            # Th√™m c·ªôt generated_ai v√† response_time\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "            df['generated_ai'] = ''\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "            df['response_time'] = ''  # ‚Üê TH√äM D√íNG N√ÄY\r\n"
            },
            {
              "count": 10,
              "added": false,
              "removed": false,
              "value": "            \r\n            # X·ª≠ l√Ω t·ª´ng row\r\n            for index, row in df.iterrows():\r\n                print(f\"üîÑ ƒêang x·ª≠ l√Ω row {index + 1}/{len(df)}\")\r\n                \r\n                conv_str = row['BOT_RESPONSE_CONVERSATION_with_USER']\r\n                conversations = self.parse_conversation_string(conv_str)\r\n                \r\n                if conversations:\r\n                    # G·ªçi API\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": true,
              "value": "                    generated_response = self.call_fast_response_api(conversations)\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "                    generated_response, response_time = self.call_fast_response_api(conversations)  # ‚Üê S·ª¨A D√íNG N√ÄY\r\n"
            },
            {
              "count": 1,
              "added": false,
              "removed": false,
              "value": "                    df.at[index, 'generated_ai'] = generated_response\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "                    df.at[index, 'response_time'] = response_time  # ‚Üê TH√äM D√íNG N√ÄY\r\n"
            },
            {
              "count": 5,
              "added": false,
              "removed": false,
              "value": "                    \r\n                    # Delay ƒë·ªÉ tr√°nh spam API\r\n                    time.sleep(0.5)\r\n                else:\r\n                    df.at[index, 'generated_ai'] = \"PARSE_ERROR\"\r\n"
            },
            {
              "count": 1,
              "added": true,
              "removed": false,
              "value": "                    df.at[index, 'response_time'] = 0  # ‚Üê TH√äM D√íNG N√ÄY\r\n"
            },
            {
              "count": 39,
              "added": false,
              "removed": false,
              "value": "                \r\n                # In progress m·ªói 10 rows\r\n                if (index + 1) % 10 == 0:\r\n                    print(f\"‚úÖ ƒê√£ ho√†n th√†nh {index + 1}/{len(df)} rows\")\r\n            \r\n            # L∆∞u file output\r\n            df.to_excel(output_filepath, index=False, engine='openpyxl')\r\n            print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ ra: {output_filepath}\")\r\n            \r\n            return True\r\n            \r\n        except Exception as e:\r\n            print(f\"‚ùå L·ªói khi ƒë√°nh gi√° file: {e}\")\r\n            return False\r\n    \r\n    def evaluate_all_processed_files(self):\r\n        \"\"\"\r\n        ƒê√°nh gi√° t·∫•t c·∫£ file processed trong folder output\r\n        \"\"\"\r\n        if not os.path.exists('output'):\r\n            print(\"‚ùå Folder 'output' kh√¥ng t·ªìn t·∫°i\")\r\n            return\r\n        \r\n        # T·∫°o folder eval n·∫øu ch∆∞a c√≥\r\n        if not os.path.exists('eval'):\r\n            os.makedirs('eval')\r\n        \r\n        for filename in os.listdir('output'):\r\n            if filename.endswith('_processed.xlsx'):\r\n                input_path = os.path.join('output', filename)\r\n                output_filename = filename.replace('_processed.xlsx', '_output_eval.xlsx')\r\n                output_path = os.path.join('eval', output_filename)\r\n                \r\n                print(f\"\\nüîÑ ƒêang ƒë√°nh gi√°: {filename}\")\r\n                self.evaluate_excel_file(input_path, output_path)\r\n\r\nif __name__ == \"__main__\":\r\n    evaluator = FastResponseEvaluator()\r\n    evaluator.evaluate_all_processed_files()\r\n"
            }
          ]
        }
      ],
      "selection": null,
      "timestamp": "2025-06-24T05:05:06.394Z",
      "prompt": "summary_data.append({\n    'ID': result['id'],\n    'Fetch Status': result['fetch_status'],\n    'Process Status': result['process_status'],\n    'Eval Status': result['eval_status'],\n    'Input File': result['input_file'],\n    'Processed File': result['processed_file'],\n    'Eval File': result['eval_file'],\n    'Avg Response Time (ms)': self.calculate_avg_response_time(result['eval_file']) if result['eval_status'] == 'SUCCESS' else 0  # ‚Üê TH√äM D√íNG N√ÄY (optional)\n})",
      "parent_inline_chat_id": null
    }
  ],
  "schemaVersion": "1.0",
  "deletedChats": {
    "deletedMessageIDs": [],
    "deletedPanelChatIDs": []
  },
  "kv_store": {
    "unique_matched_lines_count": 128,
    "total_repo_line_count": 365992,
    "best_prompt_response": null,
    "file_statistics": [
      {
        "file": ".env.example",
        "total_lines": 3,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_DeepDefine_Scoring_Benchmark_ver1.ipynb",
        "total_lines": 931,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_DeepDefine_Scoring_Benchmark_ver2_SUMMARY.ipynb",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/Audio_VideoMentor.xlsx",
        "total_lines": 3792,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/ContentAppTheCoach.md",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/IELTS Coach + AppTheCoach - Prompt order (main).xlsx",
        "total_lines": 10136,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/IELTS Coach - Prompt order (main).xlsx",
        "total_lines": 11590,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/IELTS.xlsx",
        "total_lines": 13633,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/Import_TheCoach.md",
        "total_lines": 11,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/MentorVideoScoring.md",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/genTinhHuongAppTheCoach_cTrang/10SituationsAppTheCoach.xlsx",
        "total_lines": 1511,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/genTinhHuongAppTheCoach_cTrang/TOFU_ AI_COACH.xlsx",
        "total_lines": 18072,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Log_Version.md",
        "total_lines": 48,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/README.md",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Ver1_BuildFlow_[BUT_Should_Stepbystep]_.ipynb",
        "total_lines": 776,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Ver2_Update_STEPBYSTEP.ipynb",
        "total_lines": 2439,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Ver2_[Test_ver]_MiniSituation.ipynb",
        "total_lines": 543,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Ver3_[RUN_ver]_FullSituation.ipynb",
        "total_lines": 1564,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Ver4_Output_19Jobs_merged_file.xlsx",
        "total_lines": 336,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Ver4_[RUN_ver]_Excep_Freelancer.ipynb",
        "total_lines": 1246,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Ver4_[RUN_ver]_Excep_Student.ipynb",
        "total_lines": 476,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/Ver4_[RUN_ver]_delta_Requirements.ipynb",
        "total_lines": 16305,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/NB_Prompting_GenSituationEnglish/situation_CustomizeJobSituations_CodeBuild/VerCode3_Output1_3_jobs_situations_17min14s.xlsx",
        "total_lines": 309,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/TOFU_ AI_COACH.xlsx",
        "total_lines": 18289,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/TOFU_ AI_COACH_DEV.xlsx",
        "total_lines": 17208,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/Video Mentor - DoanCuong_ver2.4.yml",
        "total_lines": 1490,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.1_Prompt/crawlMoxie_auto90%.xlsx",
        "total_lines": 994,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.2_Workflow/1_Video-Mentor-DoanCuong.yml",
        "total_lines": 1490,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.2_Workflow/HowToCode_inWorkflow_Dify.md",
        "total_lines": 58,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.3_RolePlay/CodeGiaiNenZip.py",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.3_RolePlay/note1.1_research.ipynb",
        "total_lines": 1555,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.3_RolePlay/note1.2_DeepResearch.ipynb",
        "total_lines": 86,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/1.3_RolePlay/read.md",
        "total_lines": 25,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/Template_PromptingBuilder1_JSONFormatOutput.md",
        "total_lines": 110,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "1_SomeRealPrompting_And_Workflow/Template_PromptingBuilder1_RolePlay.md",
        "total_lines": 30,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/1_PromptCreateTestingRAG_CSKH_TheCoach/lark_TestEssayTestingRAG.md",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/1_PromptCreateTestingRAG_CSKH_TheCoach/output_TestSuite/16LoaiQuestionHitRateRAG.xlsx",
        "total_lines": 8572,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/3. CreateData_TrainingModel_FastResponseIntention/Dataset_FastResponseIntentRobot vs TTS.xlsx",
        "total_lines": 32489,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/4_CreateDataASR/Data ASR - EnglishVietnamese.xlsx",
        "total_lines": 137837,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/4_CreateDataASR/Data.txt",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/4_CreateDataASR/Dataset_ASR.ipynb",
        "total_lines": 1319,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/2PromptTune_GiaLapUserTreCon_v1.3_100PikaLesson.xlsx",
        "total_lines": 8942,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/CodeLogicExtractInstructionOfPika/Extracted_Instructions.xlsx",
        "total_lines": 97,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/CodeLogicExtractInstructionOfPika/Extracted_TaskDescriptions.xlsx",
        "total_lines": 45,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/CodeLogicExtractInstructionOfPika/extract_TaskDescription.py",
        "total_lines": 118,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/CodeLogicExtractInstructionOfPika/extract_instructions.py",
        "total_lines": 113,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/CodeLogicExtractInstructionOfPika/instruction_extractor.log",
        "total_lines": 504,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/CodeLogicExtractInstructionOfPika/read.md",
        "total_lines": 14,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/CodeLogicExtractInstructionOfPika/task_description_extractor.log",
        "total_lines": 463,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/5_GiaLapPikaDataset_forTEXT2SPEECH/send_Data_Pika_Text2Speech.xlsx",
        "total_lines": 2099,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/HowToBuild.md",
        "total_lines": 37,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_PromptCreateDATA_IMPORTANCE/codeGenData_GPT4.py",
        "total_lines": 48,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "2_Prompting&MetricsDefinition.ipynb",
        "total_lines": 1527,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "3_Prompting_Chatbot.ipynb",
        "total_lines": 270,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "4_SimulationConversationChatbot/Motivation.md",
        "total_lines": 22,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "4_SimulationConversationChatbot/agent_evaluation.py",
        "total_lines": 29,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "4_SimulationConversationChatbot/ai_generator.py",
        "total_lines": 16,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "4_SimulationConversationChatbot/api_responder.py",
        "total_lines": 21,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "4_SimulationConversationChatbot/main.py",
        "total_lines": 55,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "4_SimulationConversationChatbot/note1.1_.ipynb",
        "total_lines": 373,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/.env.example",
        "total_lines": 3,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ApiClientB.md",
        "total_lines": 108,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ApiClientB_withHISTORY.md",
        "total_lines": 149,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/CHANGLOG_HowRun.md",
        "total_lines": 198,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/Evaluation_manyModels_manyFrameworkAndLibrary/Agents_Llama70BGroqVS4o-mini.xlsx",
        "total_lines": 257,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/Evaluation_manyModels_manyFrameworkAndLibrary/Workflow_Llama370B_Gemini_4ominiOpenAI.xlsx",
        "total_lines": 310,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/Evaluation_manyModels_manyFrameworkAndLibrary/read.md",
        "total_lines": 57,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/HowRun.md",
        "total_lines": 53,
        "ai_generated_lines": 18,
        "ai_generated_percentage": 33.9622641509434
      },
      {
        "file": "5_TuningPrompting/NOTE.txt",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptTuning_Gemini_v1.py",
        "total_lines": 155,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptTuning_Groq_v1.py",
        "total_lines": 249,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptTuning_OpenAI_v1.py",
        "total_lines": 113,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptTuning_OpenAI_v2_addMessageHistory.py",
        "total_lines": 172,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptTuning_OpenAI_v3_aisuiteManyModel.py",
        "total_lines": 229,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptTuning_OpenAI_v4_v2UpdateParseArguments.py",
        "total_lines": 215,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py",
        "total_lines": 341,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptTuning_Owen2.5.py",
        "total_lines": 95,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/PromptingTuning_AppsScript_Packing",
        "total_lines": 4,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/README.md",
        "total_lines": 84,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/HowRun.md",
        "total_lines": 2,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/exampleInputOutput/postprocessed.xlsx",
        "total_lines": 947,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/exampleInputOutput/preprocess.xlsx",
        "total_lines": 1230,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/exampleInputOutput/processed.xlsx",
        "total_lines": 598,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/postprocessed.xlsx",
        "total_lines": 947,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/postprocessed_ParserJson2Excel_sameKeyJSON.py",
        "total_lines": 127,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/preprocess.xlsx",
        "total_lines": 1230,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/processed.xlsx",
        "total_lines": 628,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Parser_extractJson2Excel/processed_ParserJson2Excel_theoDifferenceKey.py",
        "total_lines": 124,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Tool_LoaiBo_DongCoTu/input_data.xlsx",
        "total_lines": 329,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Tool_LoaiBo_DongCoTu/output_data.xlsx",
        "total_lines": 125,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Tool_LoaiBo_DongCoTu/output_data_removed.xlsx",
        "total_lines": 255,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Tool_LoaiBo_DongCoTu/processing_remove_word.py",
        "total_lines": 77,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Tool_LoaiBo_DongCoTu/readme.md",
        "total_lines": 101,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/ToolExtractOutput/Tool_LoaiBo_DongCoTu/word.txt",
        "total_lines": 98,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/UI_DuKien.txt",
        "total_lines": 3,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/example/[processed]db_app_robot_public_sation_record_history.xlsx",
        "total_lines": 198,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/example/input_data - Copy.xlsx",
        "total_lines": 948,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/example/input_data.xlsx",
        "total_lines": 64,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/example/input_data_v3_dataKhachHang.xlsx",
        "total_lines": 162,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/example/input_data_v3_robotPromptDetectIntent.xlsx",
        "total_lines": 181,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/example/output_data_v3_aisuiteManyModels.xlsx",
        "total_lines": 53,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/input_data.xlsx",
        "total_lines": 5829,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/note_DoanCuong/note.ipynb",
        "total_lines": 151,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/output_data_v2.xlsx",
        "total_lines": 157,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/requirements.txt",
        "total_lines": 47,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/run.sh",
        "total_lines": 3,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/someOutput/Agents_Llama70BGroqVS4o-mini.xlsx",
        "total_lines": 1546,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/someOutput/Robot.docx",
        "total_lines": 20,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/someOutput/Workflow_Llama370B_Gemini_4ominiOpenAI.xlsx",
        "total_lines": 276,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/someOutput/prepare_Prompting_smallTrainsetv7_positive_neutral_learnmore.xlsx",
        "total_lines": 876,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/someOutput/preprocess_prepare_Prompting_smallTrainsetv7_positive_neutral_learnmore.xlsx",
        "total_lines": 1371,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "5_TuningPrompting/someOutput/processed_prepare_Prompting_smallTrainsetv7_positive_neutral_learnmore.xlsx",
        "total_lines": 252,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/CKP/GIA LAP 1106.ipynb",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/PERCENTILE.md",
        "total_lines": 57,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/example/example_input.xlsx",
        "total_lines": 85,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/example/example_input_v3.xlsx",
        "total_lines": 899,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/example/example_output.md",
        "total_lines": 331,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/example/example_output_v0.xlsx",
        "total_lines": 33,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/example/example_output_v1.xlsx",
        "total_lines": 57,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/example/example_output_v3.xlsx",
        "total_lines": 57,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/example/main_v0.py",
        "total_lines": 194,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/note_DoanCuong/note1.1_.ipynb",
        "total_lines": 139,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/note_DoanCuong/note1.2_refactor.ipynb",
        "total_lines": 269,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/note_DoanCuong/note1.3_promptB2APIResponseB.ipynb",
        "total_lines": 191,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/someOutput/2PromptTuning_GiaLapUserHVAppTheCoach.xlsx",
        "total_lines": 271,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/someOutput/id20.xlsx",
        "total_lines": 162,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/someOutput/id24.xlsx",
        "total_lines": 101,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/someOutput/id33.xlsx",
        "total_lines": 134,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/someOutput/workflow_mece_table.xlsx",
        "total_lines": 19,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/2PrompTune_GiaLapUserTreCon_v2.xlsx",
        "total_lines": 264,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/2PromptTune_GiaLapUserHVAppTheCoach_v1.1_promptPTY.xlsx",
        "total_lines": 159,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/2PromptTune_GiaLapUserHVAppTheCoach_v1.2_Robot.xlsx",
        "total_lines": 293,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/2PromptTune_GiaLapUserTreCon_v1.1_callBotID.xlsx",
        "total_lines": 262,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/2PromptTune_GiaLapUserTreCon_v1.2_callBotID.xlsx",
        "total_lines": 55,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/2PromptTune_GiaLapUserTreCon_v1.3_100PikaLesson.xlsx",
        "total_lines": 4427,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/2PromptingTuning_40Turns.xlsx",
        "total_lines": 906,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/ApiClientB.md",
        "total_lines": 167,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/CHANGELOG.md",
        "total_lines": 9,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/CodeLogicCreateRoleUser/PromptPTY1.md",
        "total_lines": 106,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/CodeLogicCreateRoleUser/output_gen_prompt.xlsx",
        "total_lines": 32,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/CodeLogicCreateRoleUser/processing.py",
        "total_lines": 46,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/CodeLogicCreateRoleUser/workflow_mece_table.xlsx",
        "total_lines": 24,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/HowRun.md",
        "total_lines": 46,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/Motivation.md",
        "total_lines": 22,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/def_ApiClientB.py",
        "total_lines": 106,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/def_MaxWorkers.py",
        "total_lines": 84,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/def_promptA.py",
        "total_lines": 35,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/def_promptB.py",
        "total_lines": 51,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/def_simulate_with_api.py",
        "total_lines": 96,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/def_simulate_with_openai.py",
        "total_lines": 89,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/export_conversations_to_excel.py",
        "total_lines": 38,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/fast_api.py",
        "total_lines": 129,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/format.md",
        "total_lines": 20,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/main.py",
        "total_lines": 166,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/main_withMaxWorkers.py",
        "total_lines": 174,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/requirements.txt",
        "total_lines": 30,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/seedAndRandom_inPrompt.md",
        "total_lines": 25,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/somePromptCreate_roleA.md",
        "total_lines": 73,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/utils_convert_roles_for_api.py",
        "total_lines": 23,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "6_TuningWith2Prompting/src/utils_export_conversations_to_excel.py",
        "total_lines": 35,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/README.md",
        "total_lines": 143,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/cmd_run.sh",
        "total_lines": 7,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/docs/docs_Prompting.md",
        "total_lines": 38,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/docs/docs_processed.md",
        "total_lines": 451,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/docs/docs_report.md",
        "total_lines": 59,
        "ai_generated_lines": 11,
        "ai_generated_percentage": 18.64406779661017
      },
      {
        "file": "7_eval_compareFastResponse/docs/docs_requirements_.md",
        "total_lines": 196,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/docs/v1.1_genspark.md",
        "total_lines": 670,
        "ai_generated_lines": 25,
        "ai_generated_percentage": 3.731343283582089
      },
      {
        "file": "7_eval_compareFastResponse/example_output/eval/conversation_8532_output_eval.xlsx",
        "total_lines": 23,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/example_output/final/fast_response_evaluation_20250624_113349.xlsx",
        "total_lines": 31,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/example_output/input/conversation_8532.json",
        "total_lines": 106,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/example_output/output/conversation_8532_processed.xlsx",
        "total_lines": 25,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/example_output/v0.1_fast_response_evaluation_20250624_142532.xlsx",
        "total_lines": 117,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/example_output/v0.2_fast_response_evaluation_20250624_113822.xlsx",
        "total_lines": 98,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/example_output/v1Prompt_fast_response_evaluation_20250624_142532.xlsx",
        "total_lines": 1224,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/example_output/v2Prompt_fast_response_evaluation_20250624_150701.xlsx",
        "total_lines": 2279,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/get_data_conversation.py",
        "total_lines": 75,
        "ai_generated_lines": 3,
        "ai_generated_percentage": 4
      },
      {
        "file": "7_eval_compareFastResponse/ids.example",
        "total_lines": 3,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/ids.txt",
        "total_lines": 2,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/ids_run1.txt",
        "total_lines": 10,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/main.py",
        "total_lines": 281,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "7_eval_compareFastResponse/processed.py",
        "total_lines": 159,
        "ai_generated_lines": 5,
        "ai_generated_percentage": 3.1446540880503147
      },
      {
        "file": "7_eval_compareFastResponse/processed_v1_1History.py",
        "total_lines": 154,
        "ai_generated_lines": 11,
        "ai_generated_percentage": 7.142857142857142
      },
      {
        "file": "7_eval_compareFastResponse/processed_v2_fullHistory_CKP.py",
        "total_lines": 154,
        "ai_generated_lines": 11,
        "ai_generated_percentage": 7.142857142857142
      },
      {
        "file": "7_eval_compareFastResponse/prompt/prompt.md",
        "total_lines": 33,
        "ai_generated_lines": 6,
        "ai_generated_percentage": 18.181818181818183
      },
      {
        "file": "7_eval_compareFastResponse/run_eval_api_fast_response.py",
        "total_lines": 139,
        "ai_generated_lines": 38,
        "ai_generated_percentage": 27.33812949640288
      },
      {
        "file": "8_Package_Agent/ApiClientB.md",
        "total_lines": 167,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/CHANGELOG.md",
        "total_lines": 9,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/HowRun.md",
        "total_lines": 28,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/Motivation.md",
        "total_lines": 22,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/format.md",
        "total_lines": 20,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/seedAndRandom_inPrompt.md",
        "total_lines": 25,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/somePromptCreate_roleA.md",
        "total_lines": 73,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/2PromptingTuning_10Turns_new_v3.xlsx",
        "total_lines": 658,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/Test.xlsx",
        "total_lines": 655,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/def_ApiClientB.cpython-312.pyc",
        "total_lines": 30,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/def_promptA.cpython-312.pyc",
        "total_lines": 16,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/def_promptB.cpython-312.pyc",
        "total_lines": 16,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/def_roleA.cpython-312.pyc",
        "total_lines": 4,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/def_roleB.cpython-312.pyc",
        "total_lines": 4,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/def_simulate_with_api.cpython-312.pyc",
        "total_lines": 27,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/def_simulate_with_openai.cpython-312.pyc",
        "total_lines": 39,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/export_conversations_to_excel.cpython-312.pyc",
        "total_lines": 14,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/utils_convert_roles_for_api.cpython-312.pyc",
        "total_lines": 8,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/__pycache__/utils_export_conversations_to_excel.cpython-312.pyc",
        "total_lines": 6,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/ckp/CHANGLOG.MD",
        "total_lines": 23,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/ckp/Difficult.txt",
        "total_lines": 4,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/ckp/read.md",
        "total_lines": 33,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/ckp/testConversation.md",
        "total_lines": 7,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/ckp/toolConvertConversation2HistoryRequest.ipynb",
        "total_lines": 750,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/ckp/toolConvertConversation2HistoryRequest.md",
        "total_lines": 62,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/def_ApiClientB.py",
        "total_lines": 106,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/def_promptA.py",
        "total_lines": 35,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/def_promptB.py",
        "total_lines": 36,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/def_simulate_with_api.py",
        "total_lines": 96,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/def_simulate_with_openai.py",
        "total_lines": 89,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/export_conversations_to_excel.py",
        "total_lines": 34,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/fast_api.py",
        "total_lines": 236,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/id27.xlsx",
        "total_lines": 44,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/id27_processed.xlsx",
        "total_lines": 42,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/id32.xlsx",
        "total_lines": 50,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/id32_processed.xlsx",
        "total_lines": 36,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/id33.xlsx",
        "total_lines": 40,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/id33_processed.xlsx",
        "total_lines": 40,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/main.py",
        "total_lines": 144,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/main_convert2Ngang.py",
        "total_lines": 154,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/result.xlsx",
        "total_lines": 26,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/utils_convert_roles_for_api.py",
        "total_lines": 23,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src/utils_export_conversations_to_excel.py",
        "total_lines": 18,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/API_B.md",
        "total_lines": 106,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/ID7vsID25.xlsx",
        "total_lines": 194,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/custom_output.xlsx",
        "total_lines": 199,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/id47agent.xlsx",
        "total_lines": 210,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/id49agent.xlsx",
        "total_lines": 225,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/idea21.xlsx",
        "total_lines": 196,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/input.xlsx",
        "total_lines": 176,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/main.py",
        "total_lines": 190,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "8_Package_Agent/src2_evaluationSameHistory/output.xlsx",
        "total_lines": 215,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "9_ProblemSolving/VanDe1.ipynb",
        "total_lines": 83,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/.md",
        "total_lines": 78,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/Build_X10_with_COMPOSER.md",
        "total_lines": 25,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/CKP/PromptTuning_OpenAI_v4_v2UpdateParseArguments.py",
        "total_lines": 215,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/FormatJSON.md",
        "total_lines": 30,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/HowRun.md",
        "total_lines": 17,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/api.py",
        "total_lines": 239,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/main.py",
        "total_lines": 221,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/requirements.txt",
        "total_lines": 7,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/setup_env.ps1",
        "total_lines": 10,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/setup_env.sh",
        "total_lines": 11,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "MiniProduct1_Input_APICall_OutputJSON_Parser/web.py",
        "total_lines": 104,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "README.md",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      },
      {
        "file": "requirements.txt",
        "total_lines": 1,
        "ai_generated_lines": 0,
        "ai_generated_percentage": 0
      }
    ]
  }
}