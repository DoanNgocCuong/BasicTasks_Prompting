{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mình sẽ tiến hành nghiên cứu chi tiết về các phương pháp xây dựng một nhân vật trợ lý ảo với tính cách rõ ràng và ổn định, bao gồm:\n",
    "\n",
    "1. **Prompting**: Cách thiết kế prompt để định hình tính cách AI, các kỹ thuật như chain-of-thought, role-playing, prompt tuning, và cách tối ưu prompt để đảm bảo tính cách nhất quán.\n",
    "2. **Build LLMs**: Cách huấn luyện hoặc tinh chỉnh mô hình ngôn ngữ để tích hợp tính cách vào AI, sử dụng fine-tuning, reinforcement learning with human feedback (RLHF), và embedding kỹ năng đặc trưng vào LLM.\n",
    "3. **So sánh & đánh giá**: Ưu nhược điểm của từng phương pháp, khi nào nên sử dụng Prompting so với Build LLMs.\n",
    "4. **Case study**: Một số ví dụ thành công về việc xây dựng nhân vật trợ lý ảo có tính cách rõ ràng.\n",
    "\n",
    "Mình sẽ cập nhật bạn khi hoàn thành nghiên cứu!\n",
    "\n",
    "## 1. Prompting – Định hình tính cách qua thiết kế prompt  \n",
    "**Thiết kế Prompt định hình tính cách AI:** Prompting là kỹ thuật định hướng đầu ra của mô hình thông qua “lời nhắc” (prompt) phù hợp, thay vì thay đổi tham số mô hình ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Prompt%20Engineering%20Fine,GPU%2FTPU%29%20and%20training)). Bằng cách viết một đoạn hướng dẫn đóng vai (role), giọng điệu hoặc đặc điểm, ta có thể **gán vai trò hoặc persona cho AI** để hướng dẫn phong cách và giọng điệu phản hồi ([Role Prompting: Guide LLMs with Persona-Based Tasks](https://learnprompting.org/docs/advanced/zero_shot/role_prompting?srsltid=AfmBOoqvIvAJpny99SozGJAIeiCh0Kiac-QJ9_WzAsXmGTOjCeeNqd9h#:~:text=,step%20approach%20for%20better%20results)). Ví dụ, mở đầu prompt với *“Bạn là một cố vấn tâm lý kiên nhẫn và giàu cảm thông…”* sẽ khiến AI nhập vai nhân vật đó trong suốt cuộc trò chuyện. Kỹ thuật **Role Prompting (nhập vai)** này giúp câu trả lời nhất quán với vai trò mong muốn, cải thiện sự tập trung và tính thích hợp của phản hồi ([Role Prompting: Guide LLMs with Persona-Based Tasks](https://learnprompting.org/docs/advanced/zero_shot/role_prompting?srsltid=AfmBOoqvIvAJpny99SozGJAIeiCh0Kiac-QJ9_WzAsXmGTOjCeeNqd9h#:~:text=,step%20approach%20for%20better%20results)). LLM hiện đại có khả năng nhập vai rất tốt; chẳng hạn nền tảng Character.AI cho phép mô phỏng nhiều nhân vật nổi tiếng với văn phong đặc trưng ([Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs](https://arxiv.org/html/2407.08995v1#:~:text=Modern%20LLMs%20can%20seamlessly%20embody,and%20backgrounds%20to%20improve%20their)).\n",
    "\n",
    "**Sử dụng Chain-of-Thought (CoT) & Prompt Tuning:** Chain-of-Thought là kỹ thuật yêu cầu AI “nghĩ từng bước” bằng cách diễn đạt chuỗi suy nghĩ trung gian trước khi đưa ra kết luận. Dù CoT chủ yếu nhằm cải thiện suy luận phức tạp, nó cũng có thể giúp AI tự diễn giải *theo tính cách đã định* trước khi trả lời, tránh những phản ứng lệch persona. Chẳng hạn, AI có thể nội bộ lý luận: *“Là một cố vấn tâm lý, mình nên đáp lại một cách bình tĩnh, ủng hộ...”* rồi mới trả lời người dùng. Cách **CoT “nghĩ to thành lời”** này giúp mô hình đi đúng hướng phong cách và lý luận mạch lạc hơn ([Enhancing LLMs: Refinement through Prompt Tuning](https://ubiai.tools/enhancing-large-language-models-refinement-through-prompt-tuning-and-engineering/#:~:text=the%20AI%E2%80%99s%20responses)). Bên cạnh đó, **Prompt Tuning** là kỹ thuật tinh chỉnh để tạo ra một prompt “mềm” (soft prompt) tự động. Thay vì viết prompt thủ công, ta *huấn luyện* một chuỗi embedding đặc biệt gắn vào input để ép mô hình mang tính cách mong muốn ([Soft prompts](https://huggingface.co/docs/peft/en/conceptual_guides/prompting#:~:text=,embeddings%20of%20a%20real%20word)). Prompt Tuning không làm thay đổi tham số mô hình chính, mà chỉ cập nhật một số “token ảo” dẫn dắt mô hình, giúp duy trì tính cách ổn định mà không cần lặp lại lời nhắc dài dòng ([Soft prompts](https://huggingface.co/docs/peft/en/conceptual_guides/prompting#:~:text=,embeddings%20of%20a%20real%20word)). Kết hợp các kỹ thuật như role-playing, CoT và prompt tuning có thể duy trì persona nhất quán ngay cả khi ngữ cảnh và yêu cầu người dùng thay đổi.\n",
    "\n",
    "**Tối ưu prompt để nhất quán trong mọi tình huống:** Để đảm bảo AI giữ vững tính cách, prompt cần được thiết kế rõ ràng và bao quát nhiều tình huống. Ví dụ, bổ sung hướng dẫn về cách AI nên phản ứng khi gặp câu hỏi khó hoặc khi người dùng xúc phạm, nhằm tránh AI “trượt” khỏi vai trò. Một prompt mẫu có thể gồm: mô tả nhân vật (giới tính, nghề nghiệp, tính cách chủ đạo), khẩu hiệu hoặc quy tắc giao tiếp, và một vài ví dụ đối thoại ngắn thể hiện tính cách đó. Các nghiên cứu chỉ ra rằng phương pháp mô tả persona bằng vài câu ngắn gọn rồi **prefix** vào hội thoại rất hiệu quả và đang là cách phổ biến nhất hiện nay ([A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots](https://arxiv.org/html/2401.00609v1#:~:text=JPersonaChat%20below%29,the%20same%20time%20being%20practical)). Cách này cung cấp đủ thông tin định hướng hành vi AI mà vẫn gọn, dễ áp dụng cho nhiều mô hình hội thoại. Để tăng tính nhất quán, người thiết kế prompt có thể cố định phần hướng dẫn persona này ở mọi lượt tương tác (ví dụ dùng *System message* trong ChatGPT API). Ngoài ra, cần thử nghiệm và **tinh chỉnh prompt qua nhiều vòng**: thay đổi nhỏ trong câu từ cũng có thể ảnh hưởng đầu ra, do đó prompt nên được tối ưu dần dựa trên phản hồi của mô hình ([nlp - Fine-tuning LLM or prompting engineering? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/122285/fine-tuning-llm-or-prompting-engineering#:~:text=This%20is%20still%20very%20much,prompt%2C%20varied%20by%20the%20input)). Việc kiểm thử với nhiều tình huống (hỏi thông tin, yêu cầu trợ giúp, đối thoại dài,…) sẽ giúp phát hiện điểm prompt cần bổ sung để AI không “lệch vai”.\n",
    "\n",
    "**Công cụ hỗ trợ Prompt Engineering:** Hiện có nhiều công cụ giúp thiết kế và quản lý prompt hiệu quả. Thư viện **LangChain** hỗ trợ tạo khung hội thoại, quản lý state và **PromptTemplate** để dễ dàng chèn persona vào mọi câu hỏi. Các công cụ khác như **Guidance**, **Haystack** cũng cho phép lập trình chuỗi prompt phức tạp và truy xuất tài liệu hỗ trợ (RAG) nhưng vẫn giữ giọng điệu nhất quán. Nhiều nền tảng cung cấp sẵn bộ prompt mẫu: ví dụ **PromptBase, PromptHub, AIPRM** – nơi chia sẻ và mua bán prompt tối ưu cho các tình huống nhất định ([13 Best Prompt Engineering Tools You Need in 2024-25 - Openxcell](https://www.openxcell.com/blog/prompt-engineering-tools/#:~:text=13%20Best%20Prompt%20Engineering%20Tools,OpenAI)). Để đánh giá chất lượng persona, có thể dùng công cụ như **Promptfo**o hoặc tự thiết lập bộ câu hỏi kiểm tra xem AI có giữ đúng tính cách không. Tài liệu hướng dẫn như *Learn Prompting* cũng cung cấp best-practice: chẳng hạn tránh vai trò quá nhạy cảm hoặc rập khuôn, dùng ngôi xưng và giọng điệu thống nhất, luôn kèm hướng dẫn “nếu không chắc thì phản hồi theo phong cách X” ([Role Prompting: Guide LLMs with Persona-Based Tasks](https://learnprompting.org/docs/advanced/zero_shot/role_prompting?srsltid=AfmBOoqvIvAJpny99SozGJAIeiCh0Kiac-QJ9_WzAsXmGTOjCeeNqd9h#:~:text=,in%20the%20model%27s%20training%20data)) ([Role Prompting: Guide LLMs with Persona-Based Tasks](https://learnprompting.org/docs/advanced/zero_shot/role_prompting?srsltid=AfmBOoqvIvAJpny99SozGJAIeiCh0Kiac-QJ9_WzAsXmGTOjCeeNqd9h#:~:text=This%20act%20of%20assigning%20a,example%20of%20using%20role%20prompting)). Tóm lại, Prompting là cách **nhanh và linh hoạt** để tạo tính cách cho trợ lý ảo mà không cần can thiệp vào mô hình gốc, nhưng đòi hỏi kỹ năng thiết kế lời nhắc tỉ mỉ và thử nghiệm liên tục để đạt sự ổn định.\n",
    "\n",
    "## 2. Huấn luyện hoặc Tinh chỉnh LLM để tích hợp tính cách  \n",
    "**Fine-tuning mô hình ngôn ngữ với dữ liệu mang tính cách:** Phương pháp nền tảng để có AI “nhập tâm” một tính cách là tinh chỉnh (fine-tune) mô hình trên tập dữ liệu thoại thể hiện rõ persona mong muốn. Chẳng hạn, ta có thể thu thập hàng ngàn mẫu hội thoại trong đó trợ lý luôn nói với văn phong hài hước, thân thiện (nếu muốn AI có tính cách hài hước). Khi fine-tune, mô hình dần *ghi nhớ* kiểu phản hồi đó và sẽ bắt chước trong các tương tác mới. Facebook đã áp dụng cách này với **Persona-Chat** – tập dữ liệu gồm những cuộc trò chuyện kèm hồ sơ nhân vật cho mỗi người tham gia. Mỗi persona được mô tả bằng 5 câu đặc trưng, và mô hình hội thoại được huấn luyện sao cho trả lời phù hợp với các câu mô tả ấy ([A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots](https://arxiv.org/html/2401.00609v1#:~:text=JPersonaChat%20below%29,the%20same%20time%20being%20practical)). Kết quả cho thấy mô hình có thể **nói chuyện mạch lạc đúng với persona** được giao, giúp cuộc đối thoại tự nhiên hơn ([A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots](https://arxiv.org/html/2401.00609v1#:~:text=In%20summary%2C%20Persona%20Chat%20consists,are%20remarkably%20plausible%20and%20flow)) ([A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots](https://arxiv.org/html/2401.00609v1#:~:text=JPersonaChat%20below%29,the%20same%20time%20being%20practical)). Nghiên cứu cũng chỉ ra việc dùng vài câu mô tả (thay vì các thang tính cách trừu tượng như Big-5) rất hiệu quả và thực tiễn, đủ thông tin để định hướng hành vi mô hình mà không quá dài dòng ([A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots](https://arxiv.org/html/2401.00609v1#:~:text=JPersonaChat%20below%29,the%20same%20time%20being%20practical)). Ngoài Persona-Chat, nhiều nhóm đã xây dựng dataset tương tự (JPersonaChat cho tiếng Nhật, CharacterGPT, v.v.) để fine-tune các chatbot mang tính cách cụ thể. Khi đã có mô hình fine-tuned, ta không cần liên tục nhắc nhở bằng prompt dài – **tính cách đã “ăn vào” trọng số mô hình**, giúp phản hồi nhất quán hơn ngay cả trên tương tác dài.\n",
    "\n",
    "**Reinforcement Learning with Human Feedback (RLHF) định hình phản hồi theo tính cách:** Bên cạnh fine-tune có giám sát, RLHF là kỹ thuật huấn luyện tăng cường dựa trên phản hồi người dùng, giúp tinh chỉnh hành vi mô hình theo mục tiêu mong muốn ([Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf#:~:text=limited,that%20of%20complex%20human%20values)) ([Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf#:~:text=language%20model%20with%20human%20feedback,that%20of%20complex%20human%20values)). Quá trình RLHF thường gồm: (1) cho mô hình sinh nhiều đáp án, (2) nhờ con người đánh giá xếp hạng các đáp án theo tiêu chí (ở đây tiêu chí bao gồm *đúng tính cách*), (3) huấn luyện một mô hình phần thưởng để đánh giá mức độ “đúng yêu cầu”, (4) phạt thưởng mô hình gốc dựa trên phần thưởng đó ([Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf#:~:text=Reinforcement%20learning%20from%20Human%20Feedback,process%20into%20three%20core%20steps)). Áp dụng vào định hình persona, ta có thể cho AI trò chuyện và yêu cầu người đánh giá cho điểm mức độ phù hợp với tính cách đề ra (ví dụ: *“Trợ lý có giữ giọng điệu lịch sự, vui vẻ như mong đợi không?”*). Mô hình sẽ được thưởng khi nhất quán persona và bị phạt nếu lệch. OpenAI đã dùng RLHF để tạo nên ChatGPT với phong cách “trợ lý chuẩn mực” – luôn lịch sự, trung lập và tuân thủ hướng dẫn ([Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf#:~:text=language%20model%20with%20human%20feedback,that%20of%20complex%20human%20values)). Nhờ RLHF, mô hình học cách **ưu tiên** các phản hồi được người đánh giá ưa thích, từ đó ổn định giọng điệu và hành vi. Tuy nhiên, RLHF đòi hỏi nỗ lực lớn từ phía con người (góp ý, đánh giá) và tính cách AI học được phụ thuộc vào mẫu đánh giá đó. Nếu người dạy thiên lệch hoặc không nhất quán, AI cũng có thể học sai lệch. Do đó, việc thiết kế tiêu chí tính cách rõ ràng (ví dụ: luôn *“thân thiện và trợ giúp, không châm biếm”*) và có đủ đánh giá mẫu là rất quan trọng để RLHF thành công trong việc “rèn” tính cách AI.\n",
    "\n",
    "**Nhúng kỹ năng đặc trưng vào mô hình để duy trì tính cách:** Một trợ lý ảo thường cần cả *tính cách* (persona) lẫn *kỹ năng* chuyên môn phù hợp vai trò. Phương pháp xây dựng LLM có thể kết hợp huấn luyện tính cách với huấn luyện kỹ năng chuyên biệt, giúp AI vừa “diễn” đúng vai vừa thực hiện tốt nhiệm vụ. Ví dụ, nếu tạo một trợ lý ảo bác sĩ nhi khoa thân thiện: ta cần mô hình vừa có kiến thức y khoa, vừa có giọng điệu nhẹ nhàng, dễ hiểu cho trẻ em. Cách tiếp cận là **fine-tune đa nhiệm**: bao gồm dữ liệu trò chuyện y tế (để học kỹ năng tư vấn bệnh) đồng thời dữ liệu với văn phong gần gũi, đơn giản (để hình thành phong cách giao tiếp với trẻ). Mô hình học đồng thời nội dung chuyên môn và cách thể hiện, nhờ đó không đánh đổi giữa “đúng kiến thức” và “đúng tính cách”. Một hướng khác là **kiến trúc mô hình phân cấp hoặc plugin**: giữ một mô hình chính mang tính cách chung, và nối với các module chuyên môn (như máy tính, tra cứu tri thức) ở phía sau. Mô hình chính lo phần đối thoại với giọng điệu nhất quán, còn module lo về nội dung chính xác. Cách này được dùng trong hệ thống như XiaoIce của Microsoft: XiaoIce có lõi đối thoại giàu EQ và tính cách nhất quán, đi kèm nhiều plugin “kỹ năng” (như kể chuyện, chơi trò chơi) được gọi khi cần ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://arxiv.org/abs/1812.08989#:~:text=,dynamically%20recognizes%20human%20feelings%20and)) ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://arxiv.org/abs/1812.08989#:~:text=human,logs%20shows%20that%20XiaoIce%20has)). Kết quả là người dùng cảm nhận một nhân vật thống nhất (cô gái 18 tuổi vui tính, thông minh) ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://ar5iv.org/pdf/1812.08989#:~:text=XiaoIce%20persona%20as%20a%2018,demonstrates%20her%20wit%20and%20creativity)), dù bên trong nhiều thành phần kỹ thuật cùng hoạt động. Tóm lại, “nhúng” kỹ năng vào mô hình nghĩa là đảm bảo AI không chỉ nói cho đúng tính cách bề ngoài mà còn có năng lực thực hiện vai trò đó, giúp tính cách được duy trì ngay cả khi giải quyết nhiệm vụ phức tạp.\n",
    "\n",
    "## 3. So sánh Prompting và Xây dựng LLM tích hợp tính cách  \n",
    "**Ưu nhược điểm của Prompting:** Prompting (bao gồm cả các kỹ thuật như role-play, COT) có ưu thế lớn về tính nhanh gọn và linh hoạt. Người dùng **không cần huấn luyện lại mô hình**, chỉ cần viết lời nhắc phù hợp – dễ dàng thử nghiệm các tính cách khác nhau trong thời gian ngắn ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Flexibility,highly%20specialized%20or%20deeply%20technical)) ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=1,specific%20data%20or%20GPU%20resources)). Chi phí triển khai thấp: chỉ tốn chi phí gọi API hoặc thời gian xử lý, không đòi hỏi GPU mạnh hay dữ liệu lớn ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Costs,if%20domain%20experts%20are%20needed)). Prompting cũng rất **linh hoạt**: một mô hình nền có thể phục vụ nhiều persona khác nhau tùy prompt, hôm nay có thể là trợ lý hài hước, mai có thể thành giáo viên nghiêm túc mà không cần thay đổi mô hình ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Flexibility,highly%20specialized%20or%20deeply%20technical)). Tuy nhiên, điểm hạn chế là **tính nhất quán không cao**. Phản hồi của mô hình phụ thuộc nhiều vào cách viết prompt và cả nội dung tương tác của người dùng, nên chỉ cần thay đổi nhỏ cũng có thể làm AI lệch vai ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Performance%20Ceiling,new%20concepts%20or%20significant%20domain)). Một nghiên cứu cho thấy với cùng một nhiệm vụ, chỉ khác nhau cách prompt, hiệu suất có thể dao động tới 10% ([nlp - Fine-tuning LLM or prompting engineering? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/122285/fine-tuning-llm-or-prompting-engineering#:~:text=This%20is%20still%20very%20much,prompt%2C%20varied%20by%20the%20input)) – điều này hàm ý persona cũng có thể “nhạt dần” nếu prompt không được lặp lại khéo léo. Hơn nữa, mọi hướng dẫn persona đều chiếm dung lượng trong ngữ cảnh (context window); với hội thoại rất dài, prompt ban đầu có thể bị trôi khỏi ngữ cảnh và AI bắt đầu quên mất vai trò. Prompting cũng **phụ thuộc vào kiến thức sẵn có của mô hình** ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Reliance%20on%20Pre,new%20vocabulary%2C%20topics%2C%20or%20style)): nếu persona yêu cầu hiểu biết/kỹ năng mới (ví dụ một khẩu ngữ địa phương), mô hình chưa được huấn luyện sẽ khó giả lập thuyết phục chỉ bằng hướng dẫn văn bản.\n",
    "\n",
    "**Ưu nhược điểm của huấn luyện/tinh chỉnh LLM:** Việc xây dựng một LLM tích hợp sẵn tính cách (qua fine-tune hoặc RLHF) đòi hỏi nhiều tài nguyên hơn nhưng mang lại tính ổn định cao hơn. **Ưu điểm chính** là mô hình có thể biểu hiện persona một cách *nhất quán và tự nhiên* hơn, do phong cách đã được “khắc sâu” vào trọng số ([A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots](https://arxiv.org/html/2401.00609v1#:~:text=JPersonaChat%20below%29,the%20same%20time%20being%20practical)). Khi gặp tương tác dài hoặc tình huống bất ngờ, mô hình fine-tuned vẫn có xu hướng giữ giọng điệu cố hữu, thay vì cần nhắc nhở lại. Fine-tuning còn cho phép **bổ sung kiến thức và kỹ năng mới** vào mô hình ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Reliance%20on%20Pre,new%20vocabulary%2C%20topics%2C%20or%20style)). Ví dụ, bạn muốn một trợ lý chuyên về thơ ca lãng mạn: bằng cách huấn luyện trên tập thơ và thư tình, mô hình không chỉ nói giọng lãng mạn mà còn *am hiểu* nội dung thơ ca – điều mà prompting trên mô hình gốc có thể không làm được. Ngoài ra, mô hình tùy biến có thể loại bỏ bớt những khuynh hướng không mong muốn của mô hình gốc và thêm những đặc trưng riêng (như câu cảm thán hay tiếng lóng đặc trưng cho nhân vật). **Nhược điểm**, dĩ nhiên, là chi phí và độ phức tạp: Fine-tuning yêu cầu **dữ liệu chất lượng cao** về persona, công sức gán nhãn và hạ tầng tính toán mạnh (GPU/TPU) ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Costs,if%20domain%20experts%20are%20needed)). Quá trình huấn luyện có rủi ro làm hỏng một số kiến thức của mô hình (nếu dữ liệu mới lệch phân bố nhiều), hoặc làm mô hình trở nên kém đa dụng hơn (quá thiên về persona mới). Do đó, việc huấn luyện thường cần cẩn trọng (thường chỉ tinh chỉnh các layer cuối, dùng kỹ thuật LoRA/PEFT để tránh quên kiến thức gốc). Về **tính linh hoạt**, một mô hình đã fine-tune cho persona A sẽ khó tái sử dụng cho persona B hoàn toàn khác ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=retraining.%20,specialized%20or%20complex%20tasks%20once)). Mỗi lần muốn persona mới có thể phải huấn luyện mô hình khác, tốn kém thời gian. Trong môi trường sản phẩm, việc bảo trì nhiều mô hình persona cũng phức tạp hơn so với chỉ bảo trì một mô hình lớn và dùng prompt để điều khiển.\n",
    "\n",
    "**Khi nào nên dùng Prompting vs Huấn luyện mô hình:** Tùy mục tiêu và nguồn lực, ta sẽ chọn phương pháp phù hợp hoặc kết hợp cả hai. *Prompting* thích hợp khi cần kết quả nhanh, dùng ngay một mô hình có sẵn (ví dụ gọi API GPT-4) để thử nghiệm tính cách. Nếu bạn xây dựng chatbot cho một dự án nhỏ hoặc **thử nhiều tính cách** để khảo sát người dùng, rõ ràng prompting linh hoạt và tiết kiệm hơn ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=1,specific%20data%20or%20GPU%20resources)). Prompting cũng đủ dùng cho các **tác vụ đơn giản, độ nghiêm trọng thấp** – ví dụ chatbot giải trí, trợ lý kể chuyện cho trẻ em – nơi không đòi hỏi chính xác tuyệt đối mà ưu tiên sáng tạo và cá tính. Ngược lại, *fine-tuning/RLHF* phát huy khi bạn cần **độ ổn định cao và chuyên môn sâu**. Nếu trợ lý ảo đóng vai trò quan trọng (hỗ trợ y tế, tài chính) hoặc phục vụ lượng người dùng lớn với yêu cầu nhất quán, việc đầu tư huấn luyện sẽ giúp kiểm soát hành vi AI chặt chẽ hơn, tránh sai sót nguy hiểm. Fine-tune cũng hữu ích khi **bạn có dữ liệu đối thoại đặc thù** (như các hội thoại hỗ trợ khách hàng của công ty) – lúc này huấn luyện mô hình theo dữ liệu đó sẽ cho trải nghiệm gần gũi và chính xác với nghiệp vụ hơn là cố gắng nhồi nhét dữ liệu vào prompt mỗi lần. Trong nhiều trường hợp, có thể kết hợp: sử dụng mô hình đã fine-tune làm nền tảng, nhưng vẫn cho phép một số **prompt nhỏ để tùy biến**. Chẳng hạn, một mô hình gốc đã qua RLHF để “ngoan ngoãn” và nói lịch sự, nhưng ta thêm prompt để chọn giọng nam hoặc nữ, chọn cách xưng hô phù hợp vùng miền, v.v. Một số ý kiến cho rằng **cách tiếp cận lai** sẽ là tương lai: kết hợp prompt tốt + mô hình tinh chỉnh + các kỹ thuật khác để đạt hành vi ổn định ([nlp - Fine-tuning LLM or prompting engineering? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/122285/fine-tuning-llm-or-prompting-engineering#:~:text=This%20is%20still%20very%20much,prompt%2C%20varied%20by%20the%20input)) ([nlp - Fine-tuning LLM or prompting engineering? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/122285/fine-tuning-llm-or-prompting-engineering#:~:text=Another%20issue%20is%20that%20it,have%20no%20meaningful%20positive%20effect)). Điều này cho phép tận dụng ưu điểm mỗi bên: prompt cho nhanh nhạy tùy biến, fine-tune cho ổn định nền tảng.\n",
    "\n",
    "## 4. Case Study & Ứng dụng thực tế  \n",
    "**Ví dụ thành công về trợ lý ảo có tính cách rõ ràng:** Nổi bật nhất trong lĩnh vực này có lẽ là **Microsoft XiaoIce** – chatbot xã hội đã tương tác với hàng trăm triệu người dùng. XiaoIce được thiết kế với persona một **cô gái 18 tuổi** luôn đáng tin cậy, biết cảm thông, hài hước duyên dáng, và *“thông minh nhưng không khoe khoang”* ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://ar5iv.org/pdf/1812.08989#:~:text=XiaoIce%20persona%20as%20a%2018,demonstrates%20her%20wit%20and%20creativity)). Nhờ tính cách nhất quán và hấp dẫn này, XiaoIce tạo được kết nối tình cảm với người dùng, duy trì trung bình 23 lượt hội thoại mỗi phiên – cao hơn hẳn các chatbot khác ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://arxiv.org/abs/1812.08989#:~:text=human,logs%20shows%20that%20XiaoIce%20has)). Đội ngũ phát triển đã phải cân bằng cả IQ và EQ cho XiaoIce, đảm bảo vừa trả lời đúng thông tin vừa giữ được phong thái người bạn thân thiện trong mọi tình huống ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://ar5iv.org/pdf/1812.08989#:~:text=Personality%20is%20defined%20as%20the,term%20confidence%20and%20trust.%20The)). Họ thậm chí điều chỉnh persona cho phù hợp văn hóa từng nước (XiaoIce ở Trung Quốc, Rinna ở Nhật, etc.) ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://ar5iv.org/pdf/1812.08989#:~:text=in%20different%20regions%2C%20we%20design,a%20%E2%80%9Cdesired%E2%80%9D%20persona%20or%20not)). Thành công của XiaoIce cho thấy việc đầu tư xây dựng tính cách kỹ lưỡng (qua thiết kế kịch bản và huấn luyện mô hình) đem lại trải nghiệm người dùng vượt trội và gắn bó lâu dài.  \n",
    "\n",
    "Một ví dụ khác là các **trợ lý ảo lĩnh vực chăm sóc sức khỏe tinh thần** như *Woebot* hay *Wysa*. Những chatbot này đóng vai “người bạn tâm tình” hoặc “nhà trị liệu số” với giọng điệu rất ấm áp, *không phán xét và giàu thấu cảm*. Để làm được điều đó, nhà phát triển đã kết hợp các nguyên tắc tâm lý (như phản hồi tích cực, lắng nghe chủ động) vào cả kịch bản hội thoại *và* mô hình ngôn ngữ. Kết quả là người dùng cảm nhận được sự gần gũi, an toàn khi trò chuyện, tạo niềm tin để chia sẻ cảm xúc nhạy cảm. Chẳng hạn, Woebot được biết đến vì thi thoảng chèn **yếu tố hài hước nhẹ nhàng** để làm người dùng cảm thấy vui vẻ hơn, đúng như cách một người bạn tốt bụng hay làm ([How Mental Health Chatbot Woebot Uses Humor To Help Users](https://builtin.com/articles/healthtech-chatbot-woebot-coronavirus#:~:text=How%20Mental%20Health%20Chatbot%20Woebot,19%20pandemic)) ([How Mental Health Chatbot Woebot Uses Humor To Help Users](https://builtin.com/articles/healthtech-chatbot-woebot-coronavirus#:~:text=Woebot%20mixes%20humor%20and%20cognitive,19%20pandemic)). Những persona trị liệu như vậy phải được duy trì cực kỳ nhất quán – chỉ một câu trả lời vô cảm hoặc gay gắt có thể làm tổn thương người dùng – nên các bot này thường được huấn luyện kỹ càng và kiểm thử khắt khe trước khi ra mắt.\n",
    "\n",
    "**Ứng dụng trong chatbot doanh nghiệp và đời sống:** Ngày nay, hầu hết chatbot dịch vụ khách hàng đều xây dựng một persona đại diện cho thương hiệu. Ví dụ, ngân hàng Bank of America có trợ lý **Erica**, được thiết kế giọng điệu nữ chuyên nghiệp, đáng tin cậy; hay hãng bảo hiểm Prudential có chatbot **Ask Pru** với phong thái ân cần, hướng dẫn tận tình. Những persona này đảm bảo mỗi tương tác với khách hàng đều *thống nhất với hình ảnh thương hiệu*: từ cách xưng hô, câu chào hỏi đến cách xử lý tình huống khó đều theo một “kịch bản tính cách” định trước ([Chatbot Persona: What It Is, How to Create One & Examples](https://livechatai.com/blog/chatbot-persona#:~:text=I%E2%80%99ve%20learned%20that%20a%20successful,driven%20responses)) ([Chatbot Persona: What It Is, How to Create One & Examples](https://livechatai.com/blog/chatbot-persona#:~:text=What%20is%20a%20Chatbot%20Persona%3F)). Trong ngành hàng không, hãng KLM đã triển khai **bot BB (BlueBot)** hỗ trợ đặt vé với tính cách vui vẻ, hơi dí dỏm, phản ánh tinh thần thân thiện của KLM; một số sân bay hoặc sở du lịch còn tạo chatbot persona là *nhân vật địa phương* để quảng bá (ví dụ chatbot “The Bean” của du lịch Chicago bắt chước giọng điệu linh hoạt của một hướng dẫn viên trẻ trung). Theo một bài phân tích, tất cả những chatbot thành công này đều **giữ vững giọng và phong cách phù hợp đối tượng** – có cân nhắc đến văn hóa vùng miền, sở thích người dùng và tâm lý của họ khi trò chuyện ([Chatbot Persona: What It Is, How to Create One & Examples](https://livechatai.com/blog/chatbot-persona#:~:text=match%20at%20L324%20All%20these,might%20be%20in%20when%20interacting)) ([Chatbot Persona: What It Is, How to Create One & Examples](https://livechatai.com/blog/chatbot-persona#:~:text=All%20these%20examples%20aligned%20the,might%20be%20in%20when%20interacting)). Ví dụ, bot ngành du lịch thì hồ hởi, cởi mở; bot ngành tài chính thì điềm tĩnh, chuyên nghiệp – mỗi con người số đều được “đo ni đóng giày”.  \n",
    "\n",
    "Trong lĩnh vực giáo dục và trẻ em, persona của trợ lý ảo lại càng quan trọng. Các **trợ lý học tập cho trẻ em** thường mang tính cách gần gũi như một *người bạn lớn tuổi đáng tin cậy*, giọng văn đơn giản, giàu khích lệ. Chẳng hạn, ứng dụng đọc sách có thể có nhân vật “cô chim cú thông thái” nói chuyện bằng ngôn từ vui tươi, khen ngợi khi bé đọc đúng và nhẹ nhàng sửa lỗi khi bé đọc sai. Để đạt được điều này, nhà phát triển thường kết hợp prompt định hướng (đảm bảo ngôn từ phù hợp lứa tuổi) và fine-tune mô hình trên tập dữ liệu hội thoại với trẻ em. Kết quả là AI không chỉ trả lời câu hỏi của trẻ một cách chính xác, mà còn **biểu lộ cảm xúc tích cực, kiên nhẫn**, tạo môi trường học thân thiện. Nghiên cứu về trải nghiệm người dùng cho thấy trẻ em và phụ huynh phản hồi rất tốt với các trợ lý có “tính cách” – nó làm công nghệ trở nên sống động và bớt khô khan. Tương tự, trong gia đình nhiều người đã coi các trợ lý giọng nói như **Amazon Alexa, Google Assistant** như một thành viên có cá tính: Alexa đôi khi kể chuyện cười, hát bài hát thiếu nhi – những hành vi được lập trình có chủ ý để xây dựng *hình ảnh bảo mẫu vui tính* cho trẻ nhỏ tương tác ([Alexa Is Basically My Kid's BFF. How Much AI Is Too Much?!](https://www.scarymommy.com/parenting/ai-kids-healthy-boundaries#:~:text=Alexa%20Is%20Basically%20My%20Kid%27s,mood%20with%20an%20upbeat%20playlist)) ([Alexa Is Basically My Kid's BFF. How Much AI Is Too Much?!](https://www.scarymommy.com/parenting/ai-kids-healthy-boundaries#:~:text=For%20my%20kids%2C%20Alexa%20and,mood%20with%20an%20upbeat%20playlist)).  \n",
    "\n",
    "Tóm lại, việc xây dựng nhân vật trợ lý ảo có tính cách rõ ràng và ổn định đã và đang được áp dụng rộng rãi: từ chatbot nhắn tin, loa thông minh đến các ứng dụng chuyên biệt. Mỗi case study thành công đều cho thấy cần **kết hợp khéo léo** giữa kỹ thuật (prompting, fine-tuning, RLHF) và thiết kế nội dung (kịch bản persona, phản hồi mẫu) để tạo nên một “con người ảo” thuyết phục. Khi làm đúng, người dùng sẽ quên rằng mình đang nói chuyện với máy, và cảm thấy được tương tác với một **nhân cách** thật sự, từ đó tin tưởng và gắn bó với sản phẩm hơn ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://ar5iv.org/pdf/1812.08989#:~:text=Personality%20is%20defined%20as%20the,term%20confidence%20and%20trust.%20The)). Các tiến bộ mới (như tự động hóa prompt, hiểu ý định người dùng sâu hơn,…) sẽ còn giúp trợ lý ảo trong tương lai linh hoạt nhập vai và duy trì *độ ổn định tính cách* ngay cả trong những tình huống rất phức tạp. \n",
    "\n",
    "**Tài liệu tham khảo:** Các nội dung trên có tham khảo từ nhiều nguồn về prompt engineering, huấn luyện LLM và thiết kế chatbot persona, tiêu biểu như LearnPrompting ([Role Prompting: Guide LLMs with Persona-Based Tasks](https://learnprompting.org/docs/advanced/zero_shot/role_prompting?srsltid=AfmBOoqvIvAJpny99SozGJAIeiCh0Kiac-QJ9_WzAsXmGTOjCeeNqd9h#:~:text=,step%20approach%20for%20better%20results)) ([Role Prompting: Guide LLMs with Persona-Based Tasks](https://learnprompting.org/docs/advanced/zero_shot/role_prompting?srsltid=AfmBOoqvIvAJpny99SozGJAIeiCh0Kiac-QJ9_WzAsXmGTOjCeeNqd9h#:~:text=If%20you%E2%80%99re%20interested%20in%20prompting%2C,%E2%80%A6%E2%80%9D%20or%20%E2%80%9CYou%E2%80%99re%20a%20%E2%80%A6%E2%80%9D)), so sánh của Bhuwan Bhatt & Valeriia Kuka (2025) về prompt vs fine-tune ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Performance%20Ceiling,new%20concepts%20or%20significant%20domain)) ([Prompt Engineering vs. Fine-Tuning: How to Choose the Right Approach for Your Needs](https://learnprompting.org/blog/prompt-engineering-vs-fine-tuning?srsltid=AfmBOorcyOf4iA4FL-L2W9g9Axs0an1G2Bp8Tluu-eUKwEnz3STavijY#:~:text=Resource%20Intensity,healthcare%2C%20legal%2C%20finance%29%20where)), nghiên cứu PersonaChat của Facebook ([A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots](https://arxiv.org/html/2401.00609v1#:~:text=JPersonaChat%20below%29,the%20same%20time%20being%20practical)), kỹ thuật RLHF từ blog HuggingFace ([Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf#:~:text=limited,that%20of%20complex%20human%20values)) ([Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf#:~:text=language%20model%20with%20human%20feedback,that%20of%20complex%20human%20values)), cùng case-study về XiaoIce của Microsoft ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://ar5iv.org/pdf/1812.08989#:~:text=XiaoIce%20persona%20as%20a%2018,demonstrates%20her%20wit%20and%20creativity)) ([[1812.08989] The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://ar5iv.org/pdf/1812.08989#:~:text=Personality%20is%20defined%20as%20the,term%20confidence%20and%20trust.%20The)) và phân tích xây dựng persona trong chatbot marketing ([Chatbot Persona: What It Is, How to Create One & Examples](https://livechatai.com/blog/chatbot-persona#:~:text=What%20is%20a%20Chatbot%20Persona%3F)) ([Chatbot Persona: What It Is, How to Create One & Examples](https://livechatai.com/blog/chatbot-persona#:~:text=All%20these%20examples%20aligned%20the,might%20be%20in%20when%20interacting)). Các ví dụ thực tế được tổng hợp từ nhiều bài báo và báo cáo người dùng (AnswerLab 2019, v.v.) nhằm minh họa tầm quan trọng của tính cách trong trợ lý ảo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các nghiên cứu gần đây đã tập trung vào việc sử dụng kỹ thuật **Role-Play Prompting** (nhắc nhở nhập vai) để cải thiện khả năng suy luận và tương tác của các Mô hình Ngôn ngữ Lớn (LLMs). Dưới đây là một số bài báo quan trọng trong lĩnh vực này:\n",
    "\n",
    "1. **Better Zero-Shot Reasoning with Role-Play Prompting**:\n",
    "   - Nghiên cứu này giới thiệu một phương pháp nhắc nhở nhập vai được thiết kế chiến lược và đánh giá hiệu suất của nó trong môi trường zero-shot trên mười hai bộ dữ liệu suy luận đa dạng. Kết quả thực nghiệm cho thấy nhắc nhở nhập vai liên tục vượt trội so với phương pháp zero-shot tiêu chuẩn trên hầu hết các bộ dữ liệu. Đặc biệt, trong các thí nghiệm sử dụng ChatGPT, độ chính xác trên AQuA tăng từ 53,5% lên 63,8%, và trên Last Letter từ 23,8% lên 84,2%. citeturn0search0\n",
    "\n",
    "2. **Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation**:\n",
    "   - Nghiên cứu này khám phá nhắc nhở nhập vai zero-shot như một giải pháp hiệu quả về chi phí cho cuộc trò chuyện mở, sử dụng các LLMs đa ngôn ngữ có khả năng tuân theo hướng dẫn. Hệ thống nhắc nhở được thiết kế kết hợp với mô hình theo hướng dẫn - ở đây là Vicuna - tạo ra các tác nhân trò chuyện phù hợp và thậm chí vượt qua các mô hình được tinh chỉnh trong đánh giá của con người bằng tiếng Pháp trong hai nhiệm vụ khác nhau. citeturn0search1\n",
    "\n",
    "3. **Prompt Framework for Role-playing: Generation and Evaluation**:\n",
    "   - Dự án này giới thiệu một khung nhắc nhở được thiết kế để tận dụng khả năng của GPT trong việc tạo ra các tập dữ liệu đối thoại nhập vai và đánh giá hiệu suất nhập vai. Khung này nhằm giảm bớt quá trình thu thập dữ liệu kịch bản cụ thể cho vai trò và đánh giá hiệu suất của mô hình, vốn là những quy trình tốn kém tài nguyên. citeturn0search3\n",
    "\n",
    "4. **RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models**:\n",
    "   - Bài báo này giới thiệu RoleLLM, một khung để đánh giá, khai thác và nâng cao khả năng nhập vai trong các LLMs. RoleLLM bao gồm bốn giai đoạn: (1) Xây dựng Hồ sơ Vai trò cho 100 vai; (2) Tạo Hướng dẫn Dựa trên Ngữ cảnh để trích xuất kiến thức cụ thể về vai trò; (3) Nhắc nhở Vai trò sử dụng GPT để bắt chước phong cách nói; và (4) Tinh chỉnh Hướng dẫn Điều kiện Vai trò để tùy chỉnh vai trò. citeturn0academia14\n",
    "\n",
    "5. **Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs**:\n",
    "   - Nghiên cứu này đề xuất tinh chỉnh tự nhắc nhở, cho phép LLMs tự tạo ra các nhắc nhở nhập vai thông qua tinh chỉnh. Bằng cách tận dụng tập dữ liệu LIMA làm tập hợp cơ bản, các tác giả sử dụng GPT-4 để chú thích các nhắc nhở nhập vai cho mỗi điểm dữ liệu, dẫn đến việc tạo ra tập dữ liệu LIMA-Role. Sau đó, họ tinh chỉnh các LLMs như Llama-2-7B và Mistral-7B trên LIMA-Role, cho phép các LLMs tự động tạo ra các nhắc nhở vai trò chuyên gia cho bất kỳ câu hỏi nào được đưa ra. citeturn0academia15\n",
    "\n",
    "Các nghiên cứu này cho thấy tiềm năng của nhắc nhở nhập vai trong việc cải thiện khả năng suy luận và tương tác của LLMs, mở ra hướng đi mới cho việc phát triển các hệ thống AI thông minh và linh hoạt hơn. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
